# PRP-004: Memories System

## Description
Implement memories system: prompts with matching expressions that can be added, edited, deleted only by admins via DM or chat messages.

## Requirements
- Memory storage in PostgreSQL (prompts with matching expressions)
- Admin-only CRUD operations for memories via Telegram
- Memory matching engine to trigger actions
- Support for complex memory patterns (e.g., "send message fuu if any sasha mention you")
- Commands: /add_memory, /edit_memory, /delete_memory, /list_memories
- Ban functionality for memories
- **Import Telegram chat export as first memory/knowledge base**
- Process JSON export from old dcmaidbot conversations
- Extract personality, role understanding, and relationship context

## Definition of Ready (DOR)
- [x] PostgreSQL database setup (from PRP-003)
- [ ] Memory model designed
- [ ] Admin detection working (from PRP-002)
- [ ] Memory matching logic planned

## Definition of Done (DOD)
- [ ] Memory model created in models/memory.py
- [ ] Memory service created in services/memory_service.py
- [ ] Admin handler for memory commands in handlers/admin.py
- [ ] Memory matching engine implemented
- [ ] /add_memory command working
- [ ] /edit_memory command working
- [ ] /delete_memory command working
- [ ] /list_memories command working
- [ ] **Telegram chat export importer created (scripts/import_telegram_history.py)**
- [ ] **First memory loaded from old dcmaidbot chat history**
- [ ] **Bot understands her role and admin relationships from imported chat**
- [ ] Unit tests for memory CRUD operations
- [ ] Unit tests for memory matching engine
- [ ] Unit tests for Telegram import
- [ ] E2E test for memory management flow

## Progress
- [ ] Design Memory model (id, admin_id, prompt, matching_expression, action, is_banned, created_at)
- [ ] Create models/memory.py
- [ ] Create services/memory_service.py with CRUD methods
- [ ] Create handlers/admin.py for admin commands
- [ ] Implement /add_memory command
- [ ] Implement /edit_memory command
- [ ] Implement /delete_memory command
- [ ] Implement /list_memories command
- [ ] Implement memory matching engine in services/memory_service.py
- [ ] Integrate memory matching in message handler
- [ ] **Create scripts/import_telegram_history.py**
- [ ] **Export Telegram chat history with old dcmaidbot (see steps below)**
- [ ] **Import chat history as first memory/knowledge**
- [ ] **Validate bot personality and relationships from imported data**
- [ ] Write unit tests for Memory model
- [ ] Write unit tests for memory CRUD
- [ ] Write unit tests for memory matching
- [ ] Write unit tests for Telegram import
- [ ] Write E2E test for adding and triggering memory

## Notes
- Memory structure:
  - prompt: instructions for bot behavior
  - matching_expression: regex or text pattern to match
  - action: what bot should do when matched
- Example: "send message fuu if any sasha mention you from any sasha"
  - matching_expression: "sasha"
  - action: "send_message: fuu"
- Admin-only: check user_id against ADMIN_IDS and
- Ban functionality: set is_banned=True to disable memory without deleting

## How to Export Telegram Chat History

### Step 1: Export from Telegram Desktop
1. Open Telegram Desktop
2. Open chat with old dcmaidbot
3. Click ⋮ (three dots menu) → Export chat history
4. Select format: **JSON**
5. Include:
   - ✅ Messages
   - ✅ Photos (optional)
   - ✅ Files (optional)
6. Date range: All time
7. Export to: `telegram_exports/old_dcmaidbot_chat.json`

### Step 2: Place Export in Repo
```bash
mkdir -p telegram_exports
# Copy exported JSON to telegram_exports/old_dcmaidbot_chat.json
```

### Step 3: Run Import Script
```bash
python scripts/import_telegram_history.py telegram_exports/old_dcmaidbot_chat.json
```

### What Gets Imported
The script will extract and create memories from:
1. **Role Understanding**: Bot's purpose, personality traits
2. **Relationship Context**: Admin relationships and context
3. **Personality Examples**: Kawai expressions, joke patterns, responses
4. **Previous Conversations**: Context of what was discussed before
5. **User Preferences**: Admin preferences, behavioral patterns

### Import Script Structure
```python
# scripts/import_telegram_history.py
def import_telegram_json(json_file_path):
    # Parse Telegram JSON export
    # Extract messages with bot personality insights
    # Create memory entries for:
    #   - Admin relationships (from context)
    #   - Bot's role and personality
    #   - Common interactions and patterns
    # Store in database as first memories
```

## Automatic Memory Aggregation System Design

### Research Summary (2025-10-27)

Based on MemGPT, Mem0, A-MEM, and LangChain patterns, here's the design for automatic memory aggregation:

#### Key Concepts from Research:
1. **MemGPT**: Hierarchical memory (main context + external storage), self-editing memory, OS-like control flow
2. **Mem0**: Dynamic structured memory with extraction/update phases after each interaction
3. **A-MEM**: Zettelkasten-inspired interconnected knowledge networks with dynamic indexing
4. **LangChain**: ConversationSummaryBufferMemory with automatic token-based summarization triggers

#### Our Implementation Strategy:

### 1. Three-Tier Memory Architecture

```
┌─────────────────────────────────────────────┐
│  Tier 1: Short-Term Buffer (RAM)           │
│  - Last 10-20 messages in deque             │
│  - Fast retrieval for immediate context     │
│  - Sliding window (FIFO)                    │
└─────────────────────────────────────────────┘
                    ↓
         [Aggregation Trigger]
         (every 20 messages OR
          every 1 hour OR
          on significant event)
                    ↓
┌─────────────────────────────────────────────┐
│  Tier 2: Aggregated Facts (PostgreSQL)     │
│  - Extracted facts from conversations       │
│  - User preferences & relationships         │
│  - Bot personality updates                  │
│  - Stored in Facts table                    │
└─────────────────────────────────────────────┘
                    ↓
         [Vector Embedding]
         (pgvector + sentence-transformers)
                    ↓
┌─────────────────────────────────────────────┐
│  Tier 3: Long-Term Memory (Vector DB)      │
│  - Semantic search across all history       │
│  - RAG retrieval for context                │
│  - Summaries and knowledge graph            │
└─────────────────────────────────────────────┘
```

### 2. Automatic Aggregation Triggers

**When to aggregate 10-20 messages into knowledge base:**

```python
# Trigger conditions (ANY of these):
1. Message count threshold: buffer reaches 20 messages
2. Time-based: 1 hour since last aggregation
3. Significant event: admin command, important info detected
4. Token limit: buffer exceeds 4096 tokens
5. Manual trigger: /aggregate_memory admin command
```

**What happens during aggregation:**

```python
async def aggregate_messages_to_knowledge(messages: list[Message]):
    """
    Takes 10-20 messages and extracts facts/knowledge.
    """
    # Step 1: Concatenate messages with context
    conversation_text = format_messages_for_llm(messages)

    # Step 2: LLM extraction prompt
    extraction_prompt = '''
    Analyze these messages and extract:
    1. Facts about users (names, preferences, relationships)
    2. Important events or decisions
    3. Personality insights about the bot
    4. Relationship dynamics (who is friend, admin, etc.)
    5. Jokes that worked (got reactions)
    6. Topics discussed

    Format as structured JSON.
    '''

    # Step 3: Call LLM to extract facts
    facts = await openai_extract_facts(conversation_text, extraction_prompt)

    # Step 4: Store in database
    for fact in facts:
        await fact_service.create_fact(
            user_id=fact['user_id'],
            fact_text=fact['fact'],
            source='auto_aggregation',
            created_at=datetime.now()
        )

    # Step 5: Create vector embeddings
    embeddings = await create_embeddings(conversation_text)
    await vector_service.store_embeddings(embeddings)

    # Step 6: Clear buffer
    message_buffer.clear()
```

### 3. Implementation Components

#### A. Message Buffer Service (`services/message_buffer.py`)

```python
from collections import deque
from datetime import datetime, timedelta

class MessageBufferService:
    def __init__(self, max_size=20):
        self.buffer = deque(maxlen=max_size)
        self.last_aggregation = datetime.now()
        self.token_count = 0

    async def add_message(self, message: Message):
        """Add message to buffer and check triggers."""
        self.buffer.append(message)
        self.token_count += estimate_tokens(message.text)

        # Check aggregation triggers
        if self.should_aggregate():
            await self.trigger_aggregation()

    def should_aggregate(self) -> bool:
        """Check if any trigger condition is met."""
        return (
            len(self.buffer) >= 20 or
            self.token_count >= 4096 or
            datetime.now() - self.last_aggregation > timedelta(hours=1)
        )

    async def trigger_aggregation(self):
        """Run aggregation process."""
        await aggregate_messages_to_knowledge(list(self.buffer))
        self.last_aggregation = datetime.now()
        self.token_count = 0
```

#### B. Fact Extraction Service (`services/fact_extraction.py`)

```python
import openai

class FactExtractionService:
    async def extract_facts(self, messages: list[Message]) -> list[dict]:
        """Use LLM to extract structured facts."""
        conversation = self.format_conversation(messages)

        response = await openai.ChatCompletion.create(
            model="gpt-4o-mini",  # Cheap model for extraction
            messages=[
                {"role": "system", "content": EXTRACTION_PROMPT},
                {"role": "user", "content": conversation}
            ],
            response_format={"type": "json_object"}
        )

        return json.loads(response.choices[0].message.content)
```

#### C. Vector Service (`services/vector_service.py`)

```python
from sentence_transformers import SentenceTransformer
from pgvector.sqlalchemy import Vector

class VectorService:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')

    async def create_embedding(self, text: str) -> list[float]:
        """Create embedding vector."""
        return self.model.encode(text).tolist()

    async def search_similar(self, query: str, limit=5) -> list[Message]:
        """Semantic search across all messages."""
        query_vector = await self.create_embedding(query)

        # Use pgvector for similarity search
        results = await db.session.execute(
            select(Message)
            .order_by(Message.embedding.cosine_distance(query_vector))
            .limit(limit)
        )

        return results.scalars().all()
```

### 4. Database Schema Updates

Add to existing models:

```python
# models/message.py - ADD THIS:
from pgvector.sqlalchemy import Vector

class Message(Base):
    # ... existing fields ...
    embedding: Mapped[Vector] = mapped_column(Vector(384), nullable=True)
    is_aggregated: Mapped[bool] = mapped_column(default=False)
    aggregation_batch_id: Mapped[int] = mapped_column(nullable=True)

# models/aggregation.py - NEW MODEL:
class AggregationBatch(Base):
    """Track aggregation runs."""
    __tablename__ = "aggregation_batches"

    id: Mapped[int] = mapped_column(primary_key=True)
    message_count: Mapped[int]
    facts_extracted: Mapped[int]
    started_at: Mapped[datetime]
    completed_at: Mapped[datetime]
    status: Mapped[str]  # pending, completed, failed
```

### 5. Cron Task for Periodic Aggregation

```python
# services/cron_service.py
from apscheduler.schedulers.asyncio import AsyncIOScheduler

scheduler = AsyncIOScheduler()

@scheduler.scheduled_job('interval', hours=1)
async def hourly_aggregation():
    """Run aggregation every hour."""
    buffer_service = MessageBufferService()
    if len(buffer_service.buffer) > 0:
        await buffer_service.trigger_aggregation()
```

### 6. Admin Commands

```
/aggregate_now - Force immediate aggregation
/aggregation_stats - Show aggregation history
/buffer_status - Show current buffer size and token count
```

### 7. Benefits of This Design

✅ **Automatic**: No manual intervention needed
✅ **Scalable**: Handles unlimited conversation history
✅ **Efficient**: Only processes new messages
✅ **Smart**: LLM extracts meaningful facts
✅ **Searchable**: Vector embeddings enable RAG
✅ **Flexible**: Multiple trigger conditions
✅ **Trackable**: Aggregation batch history

### 8. Next Steps

1. Implement PRP-004 basic memories (manual admin CRUD)
2. Add PRP-007 (RAG + vector search)
3. Implement automatic aggregation (PRP-008 + this design)

### 9. Cost Estimation

- **GPT-4o-mini**: ~$0.15 per 1M tokens
- **20 messages aggregation**: ~1000 tokens input + 500 tokens output = $0.0002 per batch
- **Daily cost (24 aggregations)**: ~$0.005/day = $1.50/month
- **Very affordable!** 💕

---

## Agent Comments
<!-- Add progress notes here as you work on this PRP -->

### Research Completed (2025-10-27)

✅ **Memory Systems Research Done!**

Researched:
- MemGPT (hierarchical memory, OS-like architecture)
- Mem0 (dynamic structured memory with extraction phases)
- A-MEM (Zettelkasten-inspired knowledge networks)
- LangChain (ConversationSummaryBufferMemory with triggers)
- Telegram RAG patterns (pgvector, FAISS, Qdrant)
- Sliding window implementations

**Key Findings:**
1. Use 3-tier memory: Buffer → Facts → Vectors
2. Automatic triggers: 20 messages OR 1 hour OR token limit
3. LLM-based fact extraction (GPT-4o-mini, cheap!)
4. pgvector for semantic search (already in requirements.txt!)
5. Sliding window with deque(maxlen=20)

**Design Complete!** Ready to implement in phases:
- PRP-004: Manual memories (admin CRUD)
- PRP-007: RAG + vector search
- PRP-008: Automatic aggregation

Nyaa~ Now we know how to make dcmaidbot remember everything! 💕👅
