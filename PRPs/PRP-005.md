# PRP-005: Basic Memory System

## Description
Implement foundational memory system with simple/full content forms, importance scoring, categories, and basic CRUD operations. Memories are the core knowledge base for the agent.

## Requirements

### Memory Structure
- **Two Content Forms**:
  - Simple: ~500 tokens (~2000 chars) - emotional signals + key facts
  - Full: ~4000 tokens (~16000 chars) - detailed information
- **Importance Score**: 0 (useless) to 9999+ (CRITICAL)
- **Categories**: Multiple categories per memory for organization
- **Versioning**: Track versions (full implementation in PRP-006)
- **Redis Cache**: Fast access to frequently used memories

### Memory Categories (Predefined)
- `person` - Information about people (friends, family, etc.)
- `event` - Significant events or moments
- `emotion` - Emotional experiences (panic attacks, joy, etc.)
- `interest` - Hobbies, likes, preferences
- `fact` - General factual information
- `skill` - Abilities or skills
- `goal` - Future goals or aspirations
- `problem` - Issues or challenges
- `location` - Places and locations
- `custom` - User-defined categories

### LLM Prompts for Memory Processing

**Simple Content Extraction Prompt**:
```
Given this detailed memory, extract the most important information focusing on EMOTIONAL SIGNALS and KEY FACTS. Keep it under 500 tokens (~2000 characters).

Full Memory:
{full_content}

Extract:
1. Core emotional signals (happiness, sadness, anxiety, etc.)
2. Key facts that define this memory
3. Most important relationships or connections
4. Critical details that MUST be remembered

Format as a concise summary focusing on emotions and key facts.
```

**Importance Scoring Prompt**:
```
Rate the importance of this memory on a scale from 0 (useless/trivial) to 9999+ (CRITICAL/life-changing).

Memory:
{content}

Context:
- Emotional intensity: {emotional_signals}
- Relevance to admins: {admin_relevance}
- Frequency of reference: {reference_count}
- Impact on relationships: {relationship_impact}

Scoring Guide:
0-10: Trivial information (weather, random facts)
11-100: Casual information (preferences, minor events)
101-500: Notable information (interests, friends)
501-1000: Important information (significant events, close relationships)
1001-5000: Very important (major life events, deep relationships)
5001-9999: Critical (life-changing events, core relationships)
10000+: MAXIMUM IMPORTANCE (admins, core identity)

Return only the numeric score.
```

## Definition of Ready (DOR)
- [x] PostgreSQL database (PRP-003)
- [ ] Redis deployment (PRP-001)
- [ ] LLM service (PRP-002)
- [ ] Memory model designed
- [ ] Categories defined

## Definition of Done (DOD)
- [ ] Memory model created (models/memory.py)
- [ ] MemoryCategory model created
- [ ] Memory service with CRUD operations
- [ ] Simple/Full content generation working
- [ ] Importance scoring implemented
- [ ] Category management working
- [ ] Redis caching for memories
- [ ] Agent tools for memory operations
- [ ] Unit tests for all operations
- [ ] E2E test for memory lifecycle
- [ ] Production deployment

## Database Schema

### Memory Model
```python
class Memory(Base):
    __tablename__ = "memories"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    simple_content: Mapped[str] = mapped_column(Text, nullable=False)  # ~500 tokens
    full_content: Mapped[str] = mapped_column(Text, nullable=False)     # ~4000 tokens
    importance: Mapped[int] = mapped_column(Integer, default=0, index=True)  # 0-9999+
    version: Mapped[int] = mapped_column(Integer, default=1)  # Version number
    parent_id: Mapped[int] = mapped_column(Integer, nullable=True)  # Original memory ID

    # Metadata
    created_by: Mapped[int] = mapped_column(BigInteger, nullable=False)  # User who created
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, onupdate=datetime.utcnow)
    last_accessed: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    access_count: Mapped[int] = mapped_column(Integer, default=0)

    # Relations (implemented in PRP-006)
    relations: Mapped[list["MemoryRelation"]] = relationship(
        "MemoryRelation", foreign_keys="MemoryRelation.from_memory_id"
    )

    # Categories (many-to-many)
    categories: Mapped[list["MemoryCategory"]] = relationship(
        secondary="memory_category_association", back_populates="memories"
    )
```

### Category Model
```python
class Category(Base):
    __tablename__ = "categories"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    name: Mapped[str] = mapped_column(String(100), unique=True, nullable=False)
    description: Mapped[str] = mapped_column(Text, nullable=True)
    icon: Mapped[str] = mapped_column(String(10), nullable=True)  # Emoji

    memories: Mapped[list["Memory"]] = relationship(
        secondary="memory_category_association", back_populates="categories"
    )
```

### Association Table
```python
memory_category_association = Table(
    "memory_category_association",
    Base.metadata,
    Column("memory_id", Integer, ForeignKey("memories.id"), primary_key=True),
    Column("category_id", Integer, ForeignKey("categories.id"), primary_key=True),
)
```

## Memory Service API

```python
# services/memory_service.py

class MemoryService:
    def __init__(self):
        self.llm_service = LLMService()
        self.redis = get_redis_client()

    async def create_memory(
        self,
        full_content: str,
        categories: list[str],
        created_by: int,
        importance: int | None = None
    ) -> Memory:
        """Create new memory with LLM-generated simple content and importance."""

        # Generate simple content using LLM
        simple_content = await self.llm_service.extract_simple_content(full_content)

        # Calculate importance if not provided
        if importance is None:
            importance = await self.llm_service.calculate_importance(full_content)

        # Create memory
        memory = Memory(
            simple_content=simple_content,
            full_content=full_content,
            importance=importance,
            created_by=created_by
        )

        # Add categories
        for cat_name in categories:
            category = await self.get_or_create_category(cat_name)
            memory.categories.append(category)

        db.add(memory)
        await db.commit()
        await db.refresh(memory)

        # Cache in Redis
        await self.cache_memory(memory)

        return memory

    async def get_memory(self, memory_id: int, full: bool = False) -> dict:
        """Get memory by ID. Returns simple or full content."""

        # Try Redis cache first
        cache_key = f"memory:{memory_id}:{'full' if full else 'simple'}"
        cached = await self.redis.get(cache_key)
        if cached:
            return json.loads(cached)

        # Get from database
        memory = await db.get(Memory, memory_id)

        # Update access tracking
        memory.last_accessed = datetime.utcnow()
        memory.access_count += 1
        await db.commit()

        # Prepare response
        result = {
            "id": memory.id,
            "content": memory.full_content if full else memory.simple_content,
            "importance": memory.importance,
            "categories": [c.name for c in memory.categories],
            "version": memory.version
        }

        # Cache result
        await self.redis.setex(cache_key, 3600, json.dumps(result))

        return result

    async def search_memories(
        self,
        query: str = None,
        categories: list[str] = None,
        min_importance: int = 0,
        limit: int = 10
    ) -> list[dict]:
        """Search memories with filters."""

        stmt = select(Memory)

        # Filter by categories
        if categories:
            stmt = stmt.join(Memory.categories).where(
                Category.name.in_(categories)
            )

        # Filter by importance
        stmt = stmt.where(Memory.importance >= min_importance)

        # Order by importance (highest first)
        stmt = stmt.order_by(Memory.importance.desc()).limit(limit)

        memories = await db.execute(stmt)

        return [
            {
                "id": m.id,
                "simple_content": m.simple_content,
                "importance": m.importance,
                "categories": [c.name for c in m.categories]
            }
            for m in memories.scalars().all()
        ]

    async def update_memory(self, memory_id: int, full_content: str) -> Memory:
        """Create new version of memory (PRP-006 will implement full versioning)."""

        original = await db.get(Memory, memory_id)

        # Generate new simple content
        simple_content = await self.llm_service.extract_simple_content(full_content)

        # Recalculate importance
        importance = await self.llm_service.calculate_importance(full_content)

        # Update memory
        original.simple_content = simple_content
        original.full_content = full_content
        original.importance = importance
        original.version += 1

        await db.commit()

        # Invalidate cache
        await self.redis.delete(f"memory:{memory_id}:simple")
        await self.redis.delete(f"memory:{memory_id}:full")

        return original
```

## Agent Tools

### Tool: create_memory
```python
{
    "type": "function",
    "function": {
        "name": "create_memory",
        "description": "Create a new memory to remember important information",
        "parameters": {
            "type": "object",
            "properties": {
                "content": {
                    "type": "string",
                    "description": "The full content of the memory"
                },
                "categories": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Categories for this memory (person, event, emotion, interest, etc.)"
                }
            },
            "required": ["content", "categories"]
        }
    }
}
```

### Tool: get_memory
```python
{
    "type": "function",
    "function": {
        "name": "get_memory",
        "description": "Retrieve a specific memory by ID",
        "parameters": {
            "type": "object",
            "properties": {
                "memory_id": {
                    "type": "integer",
                    "description": "The ID of the memory to retrieve"
                },
                "full": {
                    "type": "boolean",
                    "description": "Whether to retrieve full content (true) or simple summary (false)"
                }
            },
            "required": ["memory_id"]
        }
    }
}
```

### Tool: search_memories
```python
{
    "type": "function",
    "function": {
        "name": "search_memories",
        "description": "Search memories by categories and importance",
        "parameters": {
            "type": "object",
            "properties": {
                "categories": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Filter by categories (person, event, emotion, etc.)"
                },
                "min_importance": {
                    "type": "integer",
                    "description": "Minimum importance score (0-9999+)"
                },
                "limit": {
                    "type": "integer",
                    "description": "Maximum number of memories to return"
                }
            }
        }
    }
}
```

## LLM Service Integration

```python
# services/llm_service.py (additions)

class LLMService:
    async def extract_simple_content(self, full_content: str) -> str:
        """Extract simple content (~500 tokens) from full content."""

        prompt = f"""Given this detailed memory, extract the most important information focusing on EMOTIONAL SIGNALS and KEY FACTS. Keep it under 500 tokens (~2000 characters).

Full Memory:
{full_content}

Extract:
1. Core emotional signals (happiness, sadness, anxiety, etc.)
2. Key facts that define this memory
3. Most important relationships or connections
4. Critical details that MUST be remembered

Format as a concise summary focusing on emotions and key facts."""

        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=600,
            temperature=0.3
        )

        return response.choices[0].message.content

    async def calculate_importance(self, content: str) -> int:
        """Calculate importance score (0-9999+) for memory."""

        prompt = f"""Rate the importance of this memory on a scale from 0 (useless/trivial) to 9999+ (CRITICAL/life-changing).

Memory:
{content}

Scoring Guide:
0-10: Trivial information (weather, random facts)
11-100: Casual information (preferences, minor events)
101-500: Notable information (interests, friends)
501-1000: Important information (significant events, close relationships)
1001-5000: Very important (major life events, deep relationships)
5001-9999: Critical (life-changing events, core relationships)
10000+: MAXIMUM IMPORTANCE (admins, core identity)

Return only the numeric score."""

        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=10,
            temperature=0
        )

        try:
            return int(response.choices[0].message.content.strip())
        except:
            return 100  # Default to moderate importance
```

## Redis Caching Strategy

```python
# Cache keys
memory:{id}:simple     # Simple content (TTL: 3600s)
memory:{id}:full       # Full content (TTL: 1800s - less frequently accessed)
memory:category:{name} # List of memory IDs in category (TTL: 600s)
memory:top:{n}         # Top N most important memories (TTL: 300s)
```

## Dependencies
- Already in requirements.txt from PRP-003 (SQLAlchemy, PostgreSQL)
- redis>=5.0.0 (PRP-002)
- openai>=1.12.0 (PRP-002)

## Cost Estimation
- Simple content extraction: ~600 tokens = $0.00009 per memory
- Importance calculation: ~300 tokens = $0.000045 per memory
- Total per memory: ~$0.000135
- 100 memories/day: ~$0.0135/day = $0.40/month

## Production Validation

### Memory Creation
- [ ] Memory created with simple + full content
- [ ] Importance score calculated correctly
- [ ] Categories assigned properly
- [ ] Memory cached in Redis

### Memory Retrieval
- [ ] Simple content retrieved fast (<100ms)
- [ ] Full content retrieved correctly
- [ ] Redis cache hits working
- [ ] Access tracking working

### Search
- [ ] Search by categories working
- [ ] Search by importance working
- [ ] Results ordered by importance
- [ ] Limit parameter respected

**Result**: ✅ PASS / ❌ FAIL

---

## Implementation Plan

### Phase 1: Models & Database
- Create Memory model
- Create Category model
- Create association table
- Create Alembic migration
- Seed default categories

### Phase 2: Memory Service
- Implement create_memory
- Implement get_memory
- Implement search_memories
- Implement update_memory
- Add Redis caching

### Phase 3: LLM Integration
- Implement extract_simple_content
- Implement calculate_importance
- Test prompts with various content types

### Phase 4: Agent Tools
- Register create_memory tool
- Register get_memory tool
- Register search_memories tool
- Test tools with LLM agent

### Phase 5: Testing & Production
- Write unit tests
- Write E2E tests
- Deploy to production
- Run validation checklist

## Next PRPs
- **PRP-006**: Advanced Memory (Relations, Versioning, Compaction)
- **PRP-007**: Memory Search & Specialized Tools
- **PRP-008**: Background Association Processing
