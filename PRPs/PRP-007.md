# PRP-007: RAG (Retrieval-Augmented Generation) System

## Description
Implement RAG system with vector embeddings for chat history to enable context-aware responses, joke generation, and information retrieval.

## Requirements
- Vector embeddings for all messages
- Vector database integration (pgvector or separate vector DB)
- RAG search across all chat history
- Context retrieval for responses and jokes
- Integration with LLM for context-aware generation
- Semantic search capabilities

## Definition of Ready (DOR)
- [x] PostgreSQL database setup (from PRP-003)
- [ ] Vector database solution selected (pgvector vs separate DB)
- [ ] Embedding model selected (OpenAI, sentence-transformers, etc.)
- [ ] LLM API configured (OpenAI)

## Definition of Done (DOD)
- [ ] Vector database setup (pgvector or separate)
- [ ] Message embedding generation implemented
- [ ] RAG service created in services/rag_service.py
- [ ] Semantic search working across chat history
- [ ] Context retrieval for jokes implemented
- [ ] Context retrieval for responses implemented
- [ ] Integration with LLM for context-aware generation
- [ ] Unit tests for embedding generation
- [ ] Unit tests for RAG search
- [ ] E2E test for context-aware response using RAG

## Progress
- [ ] Choose vector database (pgvector extension for PostgreSQL recommended)
- [ ] Install and configure vector database
- [ ] Add embedding column to Message model
- [ ] Create services/rag_service.py
- [ ] Implement embedding generation (OpenAI embeddings or sentence-transformers)
- [ ] Implement vector storage for messages
- [ ] Implement semantic search functionality
- [ ] Implement context retrieval for joke generation
- [ ] Implement context retrieval for general responses
- [ ] Integrate RAG with LLM prompts
- [ ] Write unit tests for embedding generation
- [ ] Write unit tests for vector search
- [ ] Write unit tests for context retrieval
- [ ] Write E2E test for RAG-powered response

## Notes
- pgvector: PostgreSQL extension for vector similarity search
- Embedding models:
  - OpenAI text-embedding-ada-002 (paid, high quality)
  - sentence-transformers (free, local)
- RAG workflow:
  1. User message â†’ generate embedding
  2. Search vector DB for similar messages
  3. Retrieve top K relevant messages
  4. Include in LLM prompt for context-aware response
- Use RAG for:
  - Joke generation (find similar funny contexts)
  - General responses (understand conversation history)
  - Fact retrieval (find user-specific facts)

## Agent Comments
<!-- Add progress notes here as you work on this PRP -->
