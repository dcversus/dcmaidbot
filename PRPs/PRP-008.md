# PRP-008: Background Association Processing

## Description
Implement automatic background processing system that detects emotional signals in conversations, creates associations with existing memories, calculates importance/relation strength, and either creates new memories or updates existing ones with new connections.

## Requirements

### Emotional Signal Detection
- **Real-time Monitoring**: Analyze every message for emotional signals
- **Signal Types**: happiness, sadness, anxiety, panic, excitement, anger, love, fear, etc.
- **Intensity Scoring**: 0-100 for each detected emotion
- **Trigger Threshold**: Only process if emotional intensity > 30

### Association Creation (Background Task)
- **Async Processing**: Run in separate thread/task
- **Memory Matching**: Find related memories using semantic search
- **Root Categories**: Match to predefined categories
- **Importance Calculation**: Determine if worthy of new memory
- **Relation Strength**: Calculate connection strength

### Decision Logic
```
IF emotional_intensity > 30:
    1. Search existing memories (semantic + category)
    2. Calculate relation strength to top matches
    3. Determine importance of new information

    IF importance > 500 AND no strong existing memory:
        → Create NEW memory with relations

    ELIF strong match found (strength > 60.0):
        → Create NEW VERSION of existing memory
        → Add new relations
        → Update content with new detail

    ELSE:
        → Create weak relation to existing memory
        → Log for future reference
```

### Processing Queue
- **APScheduler**: Manage background tasks
- **Redis Queue**: Store pending processing tasks
- **Rate Limiting**: Max 5 associations/minute (avoid API costs)
- **Retry Logic**: Retry failed associations

## Architecture

### Emotional Signal Detector

```python
# services/emotion_detector.py

from enum import Enum
from dataclasses import dataclass

class EmotionType(Enum):
    HAPPINESS = "happiness"
    SADNESS = "sadness"
    ANXIETY = "anxiety"
    PANIC = "panic"
    EXCITEMENT = "excitement"
    ANGER = "anger"
    LOVE = "love"
    FEAR = "fear"
    NEUTRAL = "neutral"

@dataclass
class EmotionalSignal:
    emotion: EmotionType
    intensity: float  # 0-100
    keywords: list[str]
    context: str

class EmotionDetector:
    def __init__(self):
        self.llm_service = LLMService()

    async def detect_emotions(self, message: str) -> list[EmotionalSignal]:
        """Detect emotional signals in message."""

        prompt = f"""Analyze this message for emotional signals.

Message: {message}

For each emotion detected, provide:
1. Emotion type (happiness, sadness, anxiety, panic, excitement, anger, love, fear)
2. Intensity (0-100)
3. Key words indicating this emotion

Return as JSON array:
[
  {{"emotion": "...", "intensity": 85, "keywords": ["...", "..."], "context": "..."}}
]"""

        response = await self.llm_service.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            max_tokens=500
        )

        data = json.loads(response.choices[0].message.content)

        return [
            EmotionalSignal(
                emotion=EmotionType(e["emotion"]),
                intensity=float(e["intensity"]),
                keywords=e["keywords"],
                context=e.get("context", "")
            )
            for e in data.get("emotions", [])
        ]
```

### Association Processor

```python
# services/association_processor.py

from apscheduler.schedulers.asyncio import AsyncIOScheduler

class AssociationProcessor:
    def __init__(self):
        self.memory_service = MemoryService()
        self.emotion_detector = EmotionDetector()
        self.llm_service = LLMService()
        self.redis = get_redis_client()
        self.scheduler = AsyncIOScheduler()

        # Start background processor
        self.scheduler.add_job(
            self.process_queue,
            'interval',
            seconds=15,  # Process queue every 15 seconds
            id='association_processor'
        )
        self.scheduler.start()

    async def queue_for_processing(
        self,
        message_text: str,
        user_id: int,
        chat_id: int,
        message_id: int
    ):
        """Queue message for background association processing."""

        # Detect emotional signals
        emotions = await self.emotion_detector.detect_emotions(message_text)

        # Filter by intensity threshold
        strong_emotions = [e for e in emotions if e.intensity > 30]

        if not strong_emotions:
            return  # No strong emotions, skip

        # Add to Redis queue
        task = {
            "message_text": message_text,
            "user_id": user_id,
            "chat_id": chat_id,
            "message_id": message_id,
            "emotions": [
                {
                    "emotion": e.emotion.value,
                    "intensity": e.intensity,
                    "keywords": e.keywords,
                    "context": e.context
                }
                for e in strong_emotions
            ],
            "queued_at": datetime.utcnow().isoformat()
        }

        await self.redis.rpush("association_queue", json.dumps(task))

    async def process_queue(self):
        """Process association queue (background task)."""

        # Rate limiting: process max 5 per run
        for _ in range(5):
            task_json = await self.redis.lpop("association_queue")
            if not task_json:
                break

            task = json.loads(task_json)

            try:
                await self.process_association(task)
            except Exception as e:
                logging.error(f"Association processing failed: {e}")
                # Re-queue for retry (max 3 attempts)
                retry_count = task.get("retry_count", 0)
                if retry_count < 3:
                    task["retry_count"] = retry_count + 1
                    await self.redis.rpush("association_queue", json.dumps(task))

    async def process_association(self, task: dict):
        """Process single association task."""

        message_text = task["message_text"]
        emotions = task["emotions"]
        user_id = task["user_id"]

        # Step 1: Search existing memories (semantic search)
        related_memories = await self.memory_service.semantic_search(
            query=message_text,
            limit=10
        )

        # Step 2: Calculate relation strengths
        relations = []
        for memory in related_memories:
            strength = await self.llm_service.calculate_relation_strength(
                message_text,
                memory["simple_content"]
            )
            if strength > 20.0:  # Only keep meaningful relations
                relations.append({
                    "memory_id": memory["id"],
                    "strength": strength,
                    "memory": memory
                })

        # Sort by strength
        relations.sort(key=lambda x: x["strength"], reverse=True)

        # Step 3: Calculate importance of new information
        importance = await self.llm_service.calculate_importance(message_text)

        # Step 4: Decision logic
        strongest_relation = relations[0] if relations else None

        if importance > 500 and (not strongest_relation or strongest_relation["strength"] < 60.0):
            # CREATE NEW MEMORY
            await self.create_new_memory_with_relations(
                message_text, emotions, relations, user_id
            )

        elif strongest_relation and strongest_relation["strength"] > 60.0:
            # UPDATE EXISTING MEMORY (create new version)
            await self.update_memory_with_new_info(
                strongest_relation["memory_id"],
                message_text,
                emotions,
                relations,
                user_id
            )

        elif relations:
            # CREATE WEAK RELATIONS ONLY
            for rel in relations[:3]:  # Top 3
                await self.memory_service.create_relation(
                    from_memory_id=rel["memory_id"],
                    to_memory_id=None,  # Link to message log
                    reason=f"Mentioned in conversation: {message_text[:100]}",
                    strength=rel["strength"],
                    created_by=user_id
                )

    async def create_new_memory_with_relations(
        self,
        content: str,
        emotions: list[dict],
        relations: list[dict],
        user_id: int
    ):
        """Create new memory and connect to related memories."""

        # Determine categories based on emotions and content
        categories = await self.determine_categories(content, emotions)

        # Create memory
        memory = await self.memory_service.create_memory(
            full_content=content,
            categories=categories,
            created_by=user_id
        )

        # Create relations
        for rel in relations[:5]:  # Top 5 relations
            await self.memory_service.create_relation(
                from_memory_id=memory.id,
                to_memory_id=rel["memory_id"],
                reason=await self.llm_service.generate_relation_reason(
                    content, rel["memory"]["simple_content"]
                ),
                strength=rel["strength"],
                created_by=user_id
            )

        logging.info(f"Created new memory {memory.id} with {len(relations[:5])} relations")

    async def update_memory_with_new_info(
        self,
        memory_id: int,
        new_info: str,
        emotions: list[dict],
        relations: list[dict],
        user_id: int
    ):
        """Update existing memory with new information (create new version)."""

        # Get existing memory
        existing = await self.memory_service.get_memory(memory_id, full=True)

        # Merge new info with existing content
        merged_content = await self.llm_service.merge_memory_content(
            existing["content"],
            new_info,
            emotions
        )

        # Create new version
        new_version = await self.memory_service.create_memory_version(
            memory_id=memory_id,
            new_full_content=merged_content,
            created_by=user_id
        )

        # Add new relations
        for rel in relations[:3]:
            if rel["memory_id"] != memory_id:  # Don't relate to self
                await self.memory_service.create_relation(
                    from_memory_id=new_version.id,
                    to_memory_id=rel["memory_id"],
                    strength=rel["strength"],
                    created_by=user_id
                )

        logging.info(f"Updated memory {memory_id} -> version {new_version.version}")

    async def determine_categories(
        self,
        content: str,
        emotions: list[dict]
    ) -> list[str]:
        """Determine appropriate categories for memory."""

        prompt = f"""Determine the most appropriate categories for this memory.

Content: {content}
Emotions: {json.dumps(emotions)}

Available categories:
- person (about people)
- event (significant moments)
- emotion (emotional experiences)
- interest (hobbies, likes)
- fact (general facts)
- skill (abilities)
- goal (future goals)
- problem (issues, challenges)
- location (places)

Return 1-3 most relevant categories as JSON array: ["category1", "category2"]"""

        response = await self.llm_service.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            max_tokens=50
        )

        data = json.loads(response.choices[0].message.content)
        return data.get("categories", ["fact"])
```

### LLM Service Additions

```python
# services/llm_service.py (additions)

class LLMService:

    async def merge_memory_content(
        self,
        existing_content: str,
        new_info: str,
        emotions: list[dict]
    ) -> str:
        """Merge new information into existing memory."""

        prompt = f"""Update this memory with new information while preserving all existing details.

Existing Memory:
{existing_content}

New Information:
{new_info}

Emotional Context: {json.dumps(emotions)}

Instructions:
1. Integrate new information naturally
2. Preserve all existing facts and emotions
3. Highlight new emotional signals
4. Maintain chronological order if applicable
5. Keep under 4000 tokens

Return the updated memory content."""

        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=4500,
            temperature=0.4
        )

        return response.choices[0].message.content
```

## Integration with Message Handler

```python
# handlers/waifu.py

@router.message()
async def handle_message(message: types.Message):
    """Handle regular messages with LLM and queue for association processing."""

    # ... existing LLM response logic ...

    # Queue for background association processing
    await association_processor.queue_for_processing(
        message_text=message.text,
        user_id=message.from_user.id,
        chat_id=message.chat.id,
        message_id=message.message_id
    )
```

## Dependencies
- apscheduler>=3.10.0 (background tasks)
- Already have: redis, openai, PostgreSQL

## Cost Estimation
- Emotion detection: ~300 tokens = $0.000045
- Association processing: ~1000 tokens = $0.00015
- Total per message with emotions: ~$0.0002
- Daily (50 emotional messages): ~$0.01/day = $0.30/month

## Production Validation

### Emotional Detection
- [ ] Emotions detected correctly
- [ ] Intensity scores accurate
- [ ] Threshold filtering working (>30)

### Association Processing
- [ ] Background tasks running every 15s
- [ ] Queue processing working
- [ ] Rate limiting respected (5/run)
- [ ] Retry logic working

### Memory Creation
- [ ] New memories created for important info (>500)
- [ ] Existing memories updated with strong match (>60.0)
- [ ] Relations created correctly
- [ ] Categories assigned appropriately

**Result**: ✅ PASS / ❌ FAIL

---

## Next PRPs
- **PRP-009**: External Tools (Web search, cURL)
