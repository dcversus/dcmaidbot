# PRP-009: External Tools (Web Search & cURL)

## Description
Implement external tools integration enabling dcmaidbot to perform web searches and make HTTP requests to external APIs. Tools are available to admins and friends, with proper error handling and rate limiting.

## Requirements

### Web Search Integration
- **Search Provider**: SerpAPI or DuckDuckGo API (free alternative)
- **Query Processing**: Clean and optimize search queries
- **Result Parsing**: Extract title, snippet, URL from results
- **Limit**: Return top 5-10 most relevant results
- **Caching**: Cache search results in Redis (TTL: 1 hour)

### cURL/HTTP Request Tool
- **Methods**: GET, POST, PUT, DELETE
- **Headers**: Custom headers support
- **Authentication**: Bearer tokens, API keys, Basic Auth
- **Body**: JSON, form-data, raw text
- **Timeout**: 30 seconds max per request
- **Response Parsing**: Auto-detect JSON, HTML, text

### Access Control
- **Admins**: Full access to all tools
- **Friends**: Access with kawai/nya request
- **Others**: No access (99% ignored)
- **Logging**: Track all tool usage in database

### Safety & Rate Limiting
- **URL Allowlist**: Configurable safe domains
- **Rate Limiting**: Max 10 requests/minute per user
- **Size Limit**: Max 1MB response size
- **Timeout**: 30 seconds per request
- **Retry Logic**: Retry once on failure

## Database Schema

### ToolExecution Model
```python
class ToolExecution(Base):
    __tablename__ = "tool_executions"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    tool_name: Mapped[str] = mapped_column(String(100), nullable=False, index=True)
    user_id: Mapped[int] = mapped_column(BigInteger, nullable=False, index=True)
    chat_id: Mapped[int] = mapped_column(BigInteger, nullable=False)

    # Request details
    parameters: Mapped[str] = mapped_column(Text, nullable=False)  # JSON
    request_timestamp: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    # Response details
    response_data: Mapped[str] = mapped_column(Text, nullable=True)  # JSON
    response_timestamp: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    success: Mapped[bool] = mapped_column(Boolean, default=False)
    error_message: Mapped[str] = mapped_column(Text, nullable=True)

    # Performance
    execution_time_ms: Mapped[int] = mapped_column(Integer, nullable=True)
```

## Tool Service API

```python
# services/tool_service.py

import httpx
from serpapi import GoogleSearch
from redis import Redis

class ToolService:
    def __init__(self):
        self.redis = get_redis_client()
        self.serpapi_key = os.getenv("SERPAPI_API_KEY")
        self.http_client = httpx.AsyncClient(timeout=30.0, follow_redirects=True)

    async def web_search(
        self,
        query: str,
        num_results: int = 5,
        user_id: int = None,
        chat_id: int = None
    ) -> dict:
        """Perform web search and return top results."""

        # Check cache first
        cache_key = f"search:{hashlib.md5(query.encode()).hexdigest()}"
        cached = await self.redis.get(cache_key)
        if cached:
            return json.loads(cached)

        # Rate limiting
        await self._check_rate_limit(user_id, "web_search")

        # Log execution start
        execution = ToolExecution(
            tool_name="web_search",
            user_id=user_id,
            chat_id=chat_id,
            parameters=json.dumps({"query": query, "num_results": num_results})
        )
        db.add(execution)
        await db.commit()

        start_time = time.time()

        try:
            # Use SerpAPI if available, else DuckDuckGo
            if self.serpapi_key:
                results = await self._serpapi_search(query, num_results)
            else:
                results = await self._duckduckgo_search(query, num_results)

            execution_time = int((time.time() - start_time) * 1000)

            # Update execution record
            execution.response_data = json.dumps(results)
            execution.success = True
            execution.execution_time_ms = execution_time
            execution.response_timestamp = datetime.utcnow()
            await db.commit()

            # Cache results
            await self.redis.setex(cache_key, 3600, json.dumps(results))

            return results

        except Exception as e:
            execution_time = int((time.time() - start_time) * 1000)
            execution.success = False
            execution.error_message = str(e)
            execution.execution_time_ms = execution_time
            execution.response_timestamp = datetime.utcnow()
            await db.commit()

            raise

    async def _serpapi_search(self, query: str, num_results: int) -> dict:
        """Search using SerpAPI (paid, better results)."""

        search = GoogleSearch({
            "q": query,
            "api_key": self.serpapi_key,
            "num": num_results
        })

        results = search.get_dict()

        return {
            "query": query,
            "results": [
                {
                    "title": r.get("title"),
                    "snippet": r.get("snippet"),
                    "url": r.get("link")
                }
                for r in results.get("organic_results", [])[:num_results]
            ]
        }

    async def _duckduckgo_search(self, query: str, num_results: int) -> dict:
        """Search using DuckDuckGo (free alternative)."""

        # DuckDuckGo Instant Answer API
        response = await self.http_client.get(
            "https://api.duckduckgo.com/",
            params={
                "q": query,
                "format": "json",
                "no_html": 1
            }
        )

        data = response.json()

        results = []

        # Parse related topics
        for topic in data.get("RelatedTopics", [])[:num_results]:
            if isinstance(topic, dict) and "Text" in topic:
                results.append({
                    "title": topic.get("Text", "")[:100],
                    "snippet": topic.get("Text", ""),
                    "url": topic.get("FirstURL", "")
                })

        return {
            "query": query,
            "results": results
        }

    async def curl_request(
        self,
        url: str,
        method: str = "GET",
        headers: dict = None,
        body: str = None,
        user_id: int = None,
        chat_id: int = None
    ) -> dict:
        """Make HTTP request to external API."""

        # Validate URL
        if not await self._is_allowed_url(url):
            raise ValueError(f"URL not in allowlist: {url}")

        # Rate limiting
        await self._check_rate_limit(user_id, "curl_request")

        # Log execution
        execution = ToolExecution(
            tool_name="curl_request",
            user_id=user_id,
            chat_id=chat_id,
            parameters=json.dumps({
                "url": url,
                "method": method,
                "headers": headers,
                "body": body
            })
        )
        db.add(execution)
        await db.commit()

        start_time = time.time()

        try:
            # Make request
            response = await self.http_client.request(
                method=method,
                url=url,
                headers=headers or {},
                content=body.encode() if body else None
            )

            execution_time = int((time.time() - start_time) * 1000)

            # Check response size
            if len(response.content) > 1_000_000:  # 1MB limit
                raise ValueError("Response too large (>1MB)")

            # Parse response
            result = {
                "status_code": response.status_code,
                "headers": dict(response.headers),
                "body": response.text,
                "is_json": False
            }

            # Try to parse as JSON
            try:
                result["body"] = response.json()
                result["is_json"] = True
            except:
                pass

            # Update execution
            execution.response_data = json.dumps({
                "status": response.status_code,
                "size": len(response.content)
            })
            execution.success = True
            execution.execution_time_ms = execution_time
            execution.response_timestamp = datetime.utcnow()
            await db.commit()

            return result

        except Exception as e:
            execution_time = int((time.time() - start_time) * 1000)
            execution.success = False
            execution.error_message = str(e)
            execution.execution_time_ms = execution_time
            execution.response_timestamp = datetime.utcnow()
            await db.commit()

            raise

    async def _is_allowed_url(self, url: str) -> bool:
        """Check if URL is in allowlist."""

        # Get allowlist from Redis or config
        allowlist_key = "tool:url_allowlist"
        allowlist = await self.redis.smembers(allowlist_key)

        if not allowlist:
            # Default allowlist
            default_allowlist = [
                "api.github.com",
                "api.openai.com",
                "httpbin.org",
                "jsonplaceholder.typicode.com",
                "*.wikipedia.org"
            ]
            for domain in default_allowlist:
                await self.redis.sadd(allowlist_key, domain)
            allowlist = set(default_allowlist)

        # Parse URL
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc

        # Check exact match or wildcard
        for allowed in allowlist:
            allowed = allowed.decode() if isinstance(allowed, bytes) else allowed
            if allowed.startswith("*."):
                # Wildcard match
                if domain.endswith(allowed[2:]):
                    return True
            elif domain == allowed:
                return True

        return False

    async def _check_rate_limit(self, user_id: int, tool_name: str):
        """Check and enforce rate limiting."""

        rate_limit_key = f"ratelimit:{tool_name}:{user_id}"
        count = await self.redis.incr(rate_limit_key)

        if count == 1:
            # First request, set expiry
            await self.redis.expire(rate_limit_key, 60)

        if count > 10:
            raise ValueError(f"Rate limit exceeded: max 10 {tool_name} per minute")

    async def add_allowed_url(self, domain: str):
        """Add domain to URL allowlist (admin only)."""
        await self.redis.sadd("tool:url_allowlist", domain)

    async def remove_allowed_url(self, domain: str):
        """Remove domain from URL allowlist (admin only)."""
        await self.redis.srem("tool:url_allowlist", domain)

    async def get_allowed_urls(self) -> list[str]:
        """Get all allowed domains."""
        allowlist = await self.redis.smembers("tool:url_allowlist")
        return [
            d.decode() if isinstance(d, bytes) else d
            for d in allowlist
        ]
```

## Agent Tools

### Tool: web_search
```python
{
    "type": "function",
    "function": {
        "name": "web_search",
        "description": "Search the web for information. Returns top 5-10 results with title, snippet, and URL.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The search query"
                },
                "num_results": {
                    "type": "integer",
                    "description": "Number of results to return (default: 5, max: 10)",
                    "default": 5
                }
            },
            "required": ["query"]
        }
    }
}
```

### Tool: curl_request
```python
{
    "type": "function",
    "function": {
        "name": "curl_request",
        "description": "Make HTTP request to external API. URL must be in allowlist. Supports GET, POST, PUT, DELETE.",
        "parameters": {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "description": "The URL to request"
                },
                "method": {
                    "type": "string",
                    "enum": ["GET", "POST", "PUT", "DELETE"],
                    "description": "HTTP method",
                    "default": "GET"
                },
                "headers": {
                    "type": "object",
                    "description": "Custom headers (optional)"
                },
                "body": {
                    "type": "string",
                    "description": "Request body for POST/PUT (optional)"
                }
            },
            "required": ["url"]
        }
    }
}
```

### Admin-Only Tools

#### Tool: add_allowed_url
```python
{
    "type": "function",
    "function": {
        "name": "add_allowed_url",
        "description": "[ADMIN ONLY] Add domain to URL allowlist for curl_request",
        "parameters": {
            "type": "object",
            "properties": {
                "domain": {
                    "type": "string",
                    "description": "Domain to allow (e.g., 'api.example.com' or '*.example.com')"
                }
            },
            "required": ["domain"]
        }
    }
}
```

#### Tool: get_allowed_urls
```python
{
    "type": "function",
    "function": {
        "name": "get_allowed_urls",
        "description": "List all allowed domains for curl_request",
        "parameters": {
            "type": "object",
            "properties": {}
        }
    }
}
```

## LLM Service Integration

```python
# services/llm_service.py (additions)

class LLMService:

    async def format_search_results(
        self,
        query: str,
        results: list[dict]
    ) -> str:
        """Format search results for LLM consumption."""

        prompt = f"""Format these web search results for the query: "{query}"

Results:
{json.dumps(results, indent=2)}

Instructions:
1. Summarize the key findings
2. Highlight most relevant information
3. Include source URLs
4. Keep under 1000 tokens

Return a natural language summary."""

        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1200,
            temperature=0.3
        )

        return response.choices[0].message.content

    async def parse_api_response(
        self,
        url: str,
        response_body: str,
        is_json: bool
    ) -> str:
        """Parse and summarize API response."""

        prompt = f"""Parse this API response from {url}:

Response ({'JSON' if is_json else 'TEXT'}):
{response_body[:2000]}  # Limit to 2000 chars

Instructions:
1. Extract key data points
2. Explain what the response means
3. Note any errors or warnings
4. Keep under 500 tokens

Return a clear summary."""

        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=600,
            temperature=0.3
        )

        return response.choices[0].message.content
```

## Integration with Message Handler

```python
# handlers/waifu.py

@router.message()
async def handle_message(message: types.Message):
    """Handle messages with tool access for admins/friends."""

    user_id = message.from_user.id

    # Check if admin or friend
    is_admin = user_id in ADMIN_IDS
    is_friend = await memory_service.is_friend(user_id)

    # Detect tool request
    if is_admin or (is_friend and ("kawai" in message.text.lower() or "nya" in message.text.lower())):
        # Build tools list
        tools = [
            web_search_tool,
            curl_request_tool,
            # ... memory tools ...
        ]

        if is_admin:
            tools.extend([
                add_allowed_url_tool,
                get_allowed_urls_tool
            ])

        # Call LLM with tools
        response = await llm_service.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": BASE_PROMPT + LESSONS},
                {"role": "user", "content": message.text}
            ],
            tools=tools,
            tool_choice="auto"
        )

        # Execute tool calls
        if response.choices[0].message.tool_calls:
            for tool_call in response.choices[0].message.tool_calls:
                await execute_tool(tool_call, user_id, message.chat.id)
```

## Dependencies

```python
# requirements.txt additions
httpx>=0.27.0          # HTTP client
google-search-results>=2.4.2  # SerpAPI (optional)
```

## Environment Variables

```env
# Optional: For better search results
SERPAPI_API_KEY=your_serpapi_key

# If not provided, will use free DuckDuckGo API
```

## Cost Estimation

### With SerpAPI (paid)
- Search: $5/1000 queries
- Usage: ~50 searches/day = $0.25/day = $7.50/month

### With DuckDuckGo (free)
- Search: FREE
- cURL requests: FREE
- LLM formatting: ~1000 tokens = $0.00015 per request
- Total: ~$0.01/day = $0.30/month

**Recommendation**: Start with DuckDuckGo (free), upgrade to SerpAPI if search quality insufficient.

## Production Validation

### Web Search
- [ ] DuckDuckGo search working
- [ ] Results parsed correctly
- [ ] Cache working (Redis)
- [ ] Rate limiting enforced (10/min)
- [ ] Results formatted by LLM

### cURL Requests
- [ ] GET requests working
- [ ] POST requests with body working
- [ ] URL allowlist enforced
- [ ] Response size limit enforced (1MB)
- [ ] JSON auto-detection working
- [ ] Timeout enforced (30s)

### Access Control
- [ ] Admins have full access
- [ ] Friends can request with kawai/nya
- [ ] Non-friends blocked
- [ ] Tool executions logged in database

### Admin Tools
- [ ] add_allowed_url working
- [ ] get_allowed_urls returns list
- [ ] Only admins can modify allowlist

**Result**: ✅ PASS / ❌ FAIL

---

## Implementation Plan

### Phase 1: Tool Service Foundation
- Create ToolExecution model
- Implement ToolService class
- Add rate limiting logic
- Add URL allowlist management

### Phase 2: Web Search
- Integrate DuckDuckGo API
- Implement search result parsing
- Add Redis caching
- Test search queries

### Phase 3: cURL Requests
- Implement HTTP client with httpx
- Add URL validation
- Implement response parsing
- Add size and timeout limits

### Phase 4: LLM Integration
- Implement search result formatting
- Implement API response parsing
- Test with various queries and responses

### Phase 5: Agent Tools
- Register web_search tool
- Register curl_request tool
- Register admin-only tools
- Integrate with message handler

### Phase 6: Testing & Production
- Write unit tests for all tools
- Write E2E test for tool execution
- Deploy to production
- Run validation checklist

## Next PRPs
- **PRP-010**: Testing Framework & E2E Tests (update with new tools)
- **PRP-011**: Canary Deployment & Sister Bot Communication

## Notes
- SerpAPI is optional but provides better search results
- DuckDuckGo is free but has limited results
- URL allowlist prevents malicious requests
- Rate limiting prevents abuse and API costs
- All tool executions are logged for auditing
