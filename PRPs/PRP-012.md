# PRP-012: Analytics & Observability Integration

## Description
Implement comprehensive analytics and observability for dcmaidbot to track usage, performance, LLM interactions, user engagement, and system health. Use a combination of open-source tools optimized for self-hosted deployment and privacy-focused data collection.

## Requirements

### Analytics Stack
1. **LangSmith**: LLM tracing, cost tracking, and performance monitoring
2. **Prometheus**: Metrics collection (message counts, response times, errors)
3. **Grafana**: Visualization dashboards and alerting
4. **PostgreSQL**: Store custom analytics data (jokes reactions, user stats)

### Metrics to Track
- **Bot Performance**:
  - Message processing time
  - LLM response latency
  - API call success/failure rates
  - Memory/CPU usage

- **User Engagement**:
  - Active users (daily/weekly/monthly)
  - Message count per user
  - Command usage frequency
  - Chat activity heatmap

- **Joke Learning System**:
  - Jokes told vs reactions received
  - Joke success rate by type (setup_punchline, pun, transliteration)
  - Language-specific joke performance (ru vs en)
  - Context-based joke effectiveness

- **LLM Analytics** (via LangSmith):
  - Token usage per interaction
  - Cost per conversation
  - Model performance (GPT-4 vs GPT-3.5)
  - Prompt effectiveness
  - Error rates and types

- **Friend & Favor System**:
  - Friend interaction frequency
  - Favor request patterns
  - Tool usage by friends

- **Memory System**:
  - Memory creation/modification frequency
  - Memory match success rate
  - Admin intervention frequency

### Privacy & Compliance
- **No PII Collection**: Telegram IDs only (pseudonymous)
- **Opt-out Mechanism**: Users can disable analytics
- **Data Retention**: 90 days for raw events, 1 year for aggregated metrics
- **Admin-only Access**: Analytics dashboards restricted to admins from .env
- **GDPR Compliant**: Right to deletion, data export

## Definition of Ready (DOR)
- [ ] Research completed on analytics tools
- [ ] Analytics stack selected (LangSmith, Prometheus, Grafana)
- [ ] Database schema designed for custom analytics
- [ ] Privacy policy drafted for analytics collection
- [ ] Grafana dashboards designed (wireframes)
- [ ] Metrics naming conventions defined

## Definition of Done (DOD)
- [ ] LangSmith integration for LLM tracing
- [ ] Prometheus metrics exposed via `/metrics` endpoint
- [ ] Grafana dashboards configured with key metrics
- [ ] PostgreSQL analytics tables created
- [ ] Analytics service implemented (services/analytics_service.py)
- [ ] Privacy controls implemented (opt-out mechanism)
- [ ] Unit tests for analytics service
- [ ] E2E test for analytics pipeline
- [ ] Documentation for analytics setup
- [ ] Alerts configured in Grafana (high error rate, downtime)

## Progress

### Phase 1: Foundation
- [ ] Add dependencies (langsmith, prometheus_client, psycopg2-binary)
- [ ] Create analytics database schema (analytics_events, aggregated_stats)
- [ ] Implement analytics_service.py with event tracking
- [ ] Create Prometheus metrics registry

### Phase 2: LangSmith Integration
- [ ] Configure LangSmith API key in .env
- [ ] Wrap LLM calls with LangSmith tracing decorators
- [ ] Track joke generation prompts
- [ ] Track RAG retrieval prompts
- [ ] Monitor token usage and costs

### Phase 3: Prometheus Metrics
- [ ] Expose /metrics endpoint for Prometheus scraping
- [ ] Implement counters (messages_total, jokes_told_total)
- [ ] Implement gauges (active_users, memory_count)
- [ ] Implement histograms (message_processing_time_seconds)
- [ ] Implement summaries (llm_token_usage)

### Phase 4: Grafana Dashboards
- [ ] Create docker-compose.yml for Grafana + Prometheus
- [ ] Design dashboard: Bot Overview (messages, users, uptime)
- [ ] Design dashboard: Joke Performance (success rate by type/language)
- [ ] Design dashboard: LLM Analytics (costs, tokens, latency)
- [ ] Design dashboard: System Health (CPU, memory, errors)
- [ ] Configure alerts (error rate > 5%, downtime > 1min)

### Phase 5: Custom Analytics
- [ ] Create AnalyticsEvent model (event_type, user_id, metadata, timestamp)
- [ ] Create AggregatedStat model (metric_name, value, aggregation_period)
- [ ] Implement daily aggregation cron job
- [ ] Track joke reactions → joke_success table
- [ ] Track user engagement → user_activity table

### Phase 6: Privacy & Compliance
- [ ] Implement opt-out flag in User model (analytics_enabled)
- [ ] Create /analytics_optout command
- [ ] Implement data export for GDPR (JSON dump per user)
- [ ] Implement data deletion (90-day retention policy)
- [ ] Add analytics disclaimer to bot startup message

### Phase 7: Testing
- [ ] Unit test: track_event() method
- [ ] Unit test: Prometheus metrics registration
- [ ] Unit test: Data aggregation logic
- [ ] E2E test: Message → Analytics event → Prometheus metric
- [ ] E2E test: Opt-out mechanism
- [ ] Load test: 1000 events/second

## Notes

### LangSmith Setup
```python
import os
from langsmith import Client

os.environ["LANGSMITH_API_KEY"] = os.getenv("LANGSMITH_API_KEY")
os.environ["LANGSMITH_TRACING"] = "true"

# Automatically traces all LangChain calls
```

### Prometheus Metrics Examples
```python
from prometheus_client import Counter, Histogram, Gauge

messages_total = Counter('dcmaidbot_messages_total', 'Total messages processed', ['chat_type', 'language'])
joke_reactions = Counter('dcmaidbot_joke_reactions_total', 'Joke reactions', ['joke_type', 'reaction_type'])
processing_time = Histogram('dcmaidbot_message_processing_seconds', 'Message processing time')
active_users = Gauge('dcmaidbot_active_users', 'Currently active users')
```

### Grafana Dashboard Panels
- **Bot Overview**:
  - Messages per hour (time series)
  - Active users (gauge)
  - Joke success rate (pie chart)
  - Top commands (bar chart)

- **Joke Performance**:
  - Reactions by joke type (stacked area)
  - Language preference (pie chart)
  - Context effectiveness (heatmap)

- **LLM Analytics**:
  - Token usage over time (area chart)
  - Cost per day (line chart)
  - Prompt latency (histogram)
  - Error rate (stat panel)

### Database Schema
```sql
CREATE TABLE analytics_events (
    id SERIAL PRIMARY KEY,
    event_type VARCHAR(100) NOT NULL,  -- 'message_sent', 'joke_told', 'llm_call', etc.
    user_id INTEGER REFERENCES users(id),
    chat_id BIGINT,
    metadata JSONB,  -- Flexible storage for event-specific data
    timestamp TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_analytics_events_type_time ON analytics_events(event_type, timestamp);

CREATE TABLE aggregated_stats (
    id SERIAL PRIMARY KEY,
    metric_name VARCHAR(100) NOT NULL,
    metric_value FLOAT NOT NULL,
    aggregation_period VARCHAR(20),  -- 'hourly', 'daily', 'weekly'
    period_start TIMESTAMP NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### Privacy Configuration
```env
# Analytics (PRP-012)
LANGSMITH_API_KEY=your_langsmith_api_key
LANGSMITH_PROJECT_NAME=dcmaidbot
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000
ANALYTICS_RETENTION_DAYS=90
```

## Alternatives Considered

### PostHog (Not Selected)
- **Pros**: All-in-one (analytics + feature flags + session replay)
- **Cons**: Heavy for bot use case, designed for web/mobile apps
- **Decision**: Too complex for Telegram bot, better suited for web apps

### Umami/Plausible (Not Selected)
- **Pros**: Simple, privacy-focused, lightweight
- **Cons**: Web analytics only, not suitable for bot/LLM tracking
- **Decision**: No LLM tracing, no custom events for jokes/memories

### Why LangSmith + Prometheus + Grafana?
- **LangSmith**: Industry standard for LLM observability, LangChain native
- **Prometheus**: De facto standard for metrics, Kubernetes-native
- **Grafana**: Most popular visualization tool, rich ecosystem
- **PostgreSQL**: Already in use, no additional infrastructure
- **Lightweight**: Only 3 services (LangSmith cloud, Prometheus, Grafana local)
- **Self-hosted**: Full control, no vendor lock-in (except LangSmith)

## Agent Comments
<!-- Add progress notes here as you work on this PRP -->
