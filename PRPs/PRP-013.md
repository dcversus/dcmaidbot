# PRP-013: Production E2E Testing with Test Bot

## ✅ STATUS: COMPLETE (v0.1.0 - 2025-10-28)

**Deployed to Production**: ✅ Yes
**Version**: 0.1.0 (Status & Monitoring Dashboard)
**Last Updated**: 2025-10-28

### Production Status
- **/version endpoint**: Beautiful kawai HTML status page with system info
- **/health endpoint**: Kubernetes liveness/readiness probes working
- **Build Metadata**: Version, git commit, image tag, build timestamp tracking
- **System Runtime**: Uptime, Python version, environment, pod name display
- **Recent Changelog**: Displays from CHANGELOG.md on status page
- **Service Status**: Real-time indicators for database/Redis (⏳ ✅ ❌)

### Key Achievements
- ✅ Status & Monitoring Dashboard implemented (originally planned as PRP-013)
- ✅ Build metadata pipeline: GitHub Actions → Docker → ENV vars
- ✅ Graceful handling for pending services (database/Redis)
- ✅ Responsive terminal theme design with emoji indicators
- ✅ Version and commit hash display in /status and /help commands
- ✅ Real-time health checks added in v0.2.0
- ✅ Production E2E test suite created (11 user story tests)
- ✅ Automated testing workflow with GitHub Actions

## Description
Implement automated end-to-end production testing using a separate Telegram bot (@dcmaidbot-test) that makes real API calls to the production bot (@dcmaidbot) to verify all deployed features work correctly after each release.

## Problem Statement
Currently, we have:
- ✅ Unit tests (20 tests) - Test individual components
- ✅ E2E tests (2 tests) - Test message flows in isolation
- ❌ **No production verification** - Cannot verify deployed features work in production
- ❌ **No release smoke tests** - Manual testing after each deploy
- ❌ **No regression detection** - Features may break without detection

**Need:** Automated production testing that verifies the actual deployed bot responds correctly to real Telegram API calls.

## Solution: Test Bot Architecture

### Concept
Create a **test bot** (@dcmaidbot-test) that:
1. Uses a separate `TEST_BOT_TOKEN` to send messages to production @dcmaidbot
2. Verifies responses match expected behavior
3. Runs automatically on deploy or on-demand
4. Reports test results with detailed logs

### How It Works

```
┌─────────────────────┐
│  Test Bot Runner    │  (Python script in tests/production/)
│  @dcmaidbot-test    │
└──────────┬──────────┘
           │ 1. Send: /start
           ↓
┌─────────────────────┐
│  Production Bot     │  (Deployed to Kubernetes)
│  @dcmaidbot         │
└──────────┬──────────┘
           │ 2. Reply: "Myaw! Hello..."
           ↓
┌─────────────────────┐
│  Test Bot Runner    │
│  Verify response ✅  │
└─────────────────────┘
```

## Features to Test

### Phase 1: Core Commands (PRP-001, PRP-002)
- [x] `/start` - Waifu greeting response
- [x] `/help` - Help message with commands
- [x] `/love` - Love message for admins
- [x] `/status` - Status response
- [x] `/joke` - Joke generation
- [ ] Admin-only enforcement (non-admin receives no response or rejection)
- [ ] Kawai personality (responses contain "nya", "myaw", "💕")

### Phase 2: Database Features (PRP-003)
- [ ] Message storage (verify messages are stored in PostgreSQL)
- [ ] User tracking (verify user records created)
- [ ] Bilingual support (send Russian/English, verify correct storage)

### Phase 3: Memories System (PRP-004)
- [ ] `/add_memory` - Admin can add memory
- [ ] `/list_memories` - Admin can list memories
- [ ] `/edit_memory` - Admin can edit memory
- [ ] `/delete_memory` - Admin can delete memory
- [ ] Memory matching (trigger memory-based responses)

### Phase 4: Jokes & RAG (PRP-006, PRP-007)
- [ ] Joke generation with context
- [ ] Reaction tracking (likes on jokes)
- [ ] RAG-powered responses (context-aware replies)
- [ ] Learning from reactions (avoid no-like jokes)

### Phase 5: Cron & Tools (PRP-008, PRP-009)
- [ ] `/add_task` - Add cron task
- [ ] `/list_tasks` - List cron tasks
- [ ] Web search tool
- [ ] Game interactions

## Requirements

### 1. Test Bot Setup
- **Separate Telegram bot**: @dcmaidbot-test (or similar)
- **Environment variable**: `TEST_BOT_TOKEN` in secrets
- **Admin test account**: Test bot runs as authorized test user
- **Isolated test data**: Use separate chat/user IDs for testing

### 2. Test Framework
- **Location**: `tests/production/` directory
- **Runner**: `test_production.py` - Main test runner
- **Library**: Use `aiogram` to send messages, `pytest` for assertions
- **Async support**: All tests async using `pytest-asyncio`

### 3. Test Cases Structure

```python
# tests/production/test_production.py

import asyncio
from aiogram import Bot
from aiogram.types import Message
import pytest

@pytest.mark.asyncio
async def test_start_command():
    """Test /start command returns waifu greeting."""
    test_bot = Bot(token=TEST_BOT_TOKEN)
    prod_bot_username = "@dcmaidbot"

    # Send /start to production bot
    response = await send_command_and_wait(
        bot=test_bot,
        target=prod_bot_username,
        command="/start"
    )

    # Verify response contains expected keywords
    assert "Myaw" in response.text or "nyaw" in response.text.lower()
    assert "💕" in response.text
    assert "beloved" in response.text.lower() or "admin" in response.text.lower()
```

### 4. Test Execution

#### Manual Execution
```bash
# Run all production tests
pytest tests/production/ -v

# Run specific test
pytest tests/production/test_production.py::test_start_command -v
```

#### Automated Execution (GitHub Actions)
```yaml
# .github/workflows/production-test.yml
name: Production E2E Tests

on:
  workflow_dispatch:  # Manual trigger
  deployment_status:  # After successful deploy

jobs:
  test:
    runs-on: ubuntu-latest
    if: github.event.deployment_status.state == 'success'

    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.14'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run production tests
        env:
          TEST_BOT_TOKEN: ${{ secrets.TEST_BOT_TOKEN }}
          PROD_BOT_USERNAME: "@dcmaidbot"
        run: pytest tests/production/ -v --tb=short
```

### 5. Helper Functions

#### `send_command_and_wait()`
```python
async def send_command_and_wait(
    bot: Bot,
    target: str,
    command: str,
    timeout: int = 10
) -> Message:
    """
    Send command to target bot and wait for response.

    Args:
        bot: Test bot instance
        target: Target bot username (e.g., "@dcmaidbot")
        command: Command to send (e.g., "/start")
        timeout: Max wait time for response (seconds)

    Returns:
        Message: Response from target bot

    Raises:
        TimeoutError: If no response within timeout
    """
    # Implementation using aiogram updates listener
    pass
```

#### `verify_response_contains()`
```python
def verify_response_contains(
    response: Message,
    keywords: list[str],
    mode: str = "any"
) -> bool:
    """
    Verify response contains expected keywords.

    Args:
        response: Message from bot
        keywords: List of keywords to check
        mode: "any" or "all" - match mode

    Returns:
        bool: True if keywords found
    """
    text = response.text.lower()
    if mode == "any":
        return any(kw.lower() in text for kw in keywords)
    elif mode == "all":
        return all(kw.lower() in text for kw in keywords)
```

## Definition of Ready (DOR)
- [ ] Test bot created (@dcmaidbot-test or similar)
- [ ] `TEST_BOT_TOKEN` secret added to GitHub
- [ ] Production bot deployed and accessible (@dcmaidbot)
- [ ] PRP-003 complete (PostgreSQL for message storage verification)

## Definition of Done (DOD)
- [ ] Test framework implemented in `tests/production/`
- [ ] Test bot can send messages to production bot
- [ ] Test bot can receive and verify responses
- [ ] Phase 1 tests implemented (5 commands)
- [ ] Helper functions created (`send_command_and_wait`, `verify_response_contains`)
- [ ] GitHub Actions workflow created (`.github/workflows/production-test.yml`)
- [ ] Tests can run manually: `pytest tests/production/ -v`
- [ ] Tests can run automatically after deploy
- [ ] Documentation in README.md updated with production testing section
- [ ] All tests passing ✅
- [ ] E2E test for production test runner itself

## Test Data Management

### Test User Setup
```python
# tests/production/conftest.py

@pytest.fixture
async def test_bot():
    """Provide test bot instance."""
    return Bot(token=os.getenv("TEST_BOT_TOKEN"))

@pytest.fixture
def prod_bot_username():
    """Production bot username."""
    return os.getenv("PROD_BOT_USERNAME", "@dcmaidbot")

@pytest.fixture
async def cleanup_test_data():
    """Cleanup test data after tests."""
    yield
    # Cleanup logic here (optional)
```

### Handling Test Isolation
- Use unique test user IDs (not admin IDs)
- Test messages prefixed with `[TEST]` for easy identification
- Cleanup: Delete test messages after verification (optional)

## Test Scenarios

### Scenario 1: Deploy Verification (Smoke Test)
**When**: After each production deploy
**Tests**: Core commands only (5 tests, ~30 seconds)
**Goal**: Verify basic functionality works

### Scenario 2: Full Regression Test
**When**: Before major releases or weekly
**Tests**: All implemented features (~50 tests, ~5 minutes)
**Goal**: Verify no regressions introduced

### Scenario 3: Feature Release Test
**When**: After implementing new PRP
**Tests**: New feature tests only
**Goal**: Verify new feature works in production

## Monitoring & Reporting

### Test Results Dashboard
- Use GitHub Actions summary to display test results
- Create issue automatically if tests fail
- Slack/Telegram notification on failure

### Example Report
```
Production E2E Test Report
==========================
Date: 2025-10-27 18:30:00
Environment: Production (@dcmaidbot)
Duration: 32.5s

Results:
✅ test_start_command - PASSED (2.1s)
✅ test_help_command - PASSED (1.8s)
✅ test_love_command - PASSED (2.3s)
✅ test_status_command - PASSED (1.9s)
✅ test_joke_command - PASSED (3.2s)

Total: 5 passed, 0 failed
Status: ✅ ALL TESTS PASSED
```

## Security Considerations

### Token Protection
- `TEST_BOT_TOKEN` stored in GitHub Secrets only
- Never commit tokens to repository
- Rotate test bot token periodically

### Rate Limiting
- Implement delays between tests (1-2 seconds)
- Respect Telegram API rate limits (30 messages/second)
- Use `asyncio.sleep()` between test cases

### Privacy
- Test bot uses non-admin user ID
- No sensitive data in test messages
- Test messages clearly marked as `[TEST]`

## Implementation Plan

### Week 1: Foundation
- [x] Create test bot (@dcmaidbot-test)
- [ ] Add `TEST_BOT_TOKEN` to GitHub Secrets
- [ ] Create `tests/production/` directory structure
- [ ] Implement helper functions

### Week 2: Core Tests
- [ ] Implement Phase 1 tests (5 commands)
- [ ] Create GitHub Actions workflow
- [ ] Test manual execution
- [ ] Test automated execution

### Week 3: Advanced Tests
- [ ] Implement Phase 2 tests (database)
- [ ] Implement Phase 3 tests (memories)
- [ ] Create test results dashboard
- [ ] Documentation

## Dependencies
- **Requires**: PRP-001 (Bot deployed), PRP-002 (Commands)
- **Optional**: PRP-003 (Database verification), PRP-004 (Memories testing)
- **Blocks**: None (can run independently)

## Estimated Effort
- **Implementation**: 3-4 days
- **Testing**: 1 day
- **Documentation**: 0.5 day
- **Total**: 4-5 days (middle developer)

## Success Metrics
- ✅ Production bot verified after each deploy
- ✅ Zero manual testing required for smoke tests
- ✅ Regression detected before users report bugs
- ✅ Test execution time < 1 minute for smoke tests
- ✅ Test execution time < 10 minutes for full regression

## Future Enhancements (PRP-014+)
- Load testing (send 1000 messages/minute)
- Performance monitoring (response time tracking)
- Canary testing (@dcmaidbot-canary verification)
- Multi-environment testing (dev, staging, prod)
- Test data generation (create realistic chat scenarios)

## Related PRPs
- **PRP-001**: Infrastructure (bot deployment)
- **PRP-002**: Waifu personality (commands to test)
- **PRP-003**: PostgreSQL (database verification)
- **PRP-004**: Memories (memory testing)
- **PRP-011**: Canary deployment (canary bot testing)
- **PRP-012**: Analytics (test result metrics)

## Notes
- This PRP focuses on **production testing** of deployed bot
- Different from unit/E2E tests which test code locally
- Test bot acts as a real Telegram user interacting with production
- Critical for release confidence and regression detection

---

**Nya~ Production testing will ensure I never disappoint my beloved admins! 💕🎀**
