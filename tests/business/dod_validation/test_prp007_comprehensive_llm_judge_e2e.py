"""Comprehensive E2E Test Suite for PRP-007 with LLM Judge Validation.

This intensive test suite validates PRP-007 (Memory Search & Specialized Retrieval Tools)
through comprehensive LLM judge evaluation across multiple search patterns, edge cases,
and performance scenarios.

Test Coverage:
1. Complex query patterns with multi-concept searches
2. Cross-tool search scenarios and workflows
3. Edge cases and boundary conditions
4. Performance stress tests with LLM validation
5. Quality validation using LLM as judge
6. Real-world usage scenarios for Vasilisa and Daniil

LLM Judge Evaluation Criteria:
- Relevance: How well do results match the search intent?
- Accuracy: Are the results factually correct?
- Completeness: Do results cover all relevant memories?
- Context: Are emotional and social contexts preserved?
- Performance: Is response time acceptable?

DOD Validation:
- âœ… Vector search functionality implemented
- âœ… Embeddings generated for memories
- âœ… Semantic search returns relevant results (LLM validated)
- âœ… Specialized tools implemented and functional (LLM validated)
- âœ… Performance acceptable (<500ms for searches, LLM monitored)
"""

import json
import time
from datetime import datetime
from typing import Any, Dict, List

import pytest

from core.services.llm_service import LLMService
from core.tools.tool_executor import ToolExecutor


class LLMJudge:
    """Comprehensive LLM Judge for validating search results quality."""

    def __init__(self):
        self.llm_service = LLMService()
        self.evaluation_history = []

    async def evaluate_search_quality(
        self,
        query: str,
        results: List[Dict],
        context: str = "",
        criteria: List[str] = None,
    ) -> Dict[str, Any]:
        """
        Comprehensive evaluation of search results using LLM as judge.

        Args:
            query: The original search query
            results: List of search results
            context: Additional context for evaluation
            criteria: Evaluation criteria (relevance, accuracy, completeness, context)
        """
        if criteria is None:
            criteria = ["relevance", "accuracy", "completeness", "context"]

        # Format results for LLM evaluation
        results_text = "\n".join(
            [
                f"Result {i + 1}: {mem.get('simple_content', '')} "
                f"(Importance: {mem.get('importance', 'N/A')}, "
                f"Categories: {mem.get('categories', [])}, "
                f"Emotion: {mem.get('emotion_label', 'N/A')})"
                for i, mem in enumerate(results)
            ]
        )

        evaluation_prompt = f"""
You are an expert AI system evaluator. Analyze these search results for quality.

QUERY: "{query}"
{context}

SEARCH RESULTS:
{results_text}

Evaluate the results on a scale of 0.0 to 1.0 for each criterion:

1. RELEVANCE: How well do results match the search query intent?
2. ACCURACY: Are the results factually correct and properly categorized?
3. COMPLETENESS: Do results cover all relevant memories that should be found?
4. CONTEXT: Are emotional, social, and temporal contexts preserved appropriately?

Provide your evaluation as JSON:
{{
    "relevance": 0.0-1.0,
    "accuracy": 0.0-1.0,
    "completeness": 0.0-1.0,
    "context": 0.0-1.0,
    "overall_score": 0.0-1.0,
    "analysis": "Brief analysis of strengths and weaknesses",
    "recommendations": "Specific suggestions for improvement"
}}
"""

        try:
            response = await self.llm_service.get_response(evaluation_prompt)

            # Try to extract JSON from response
            import re

            json_match = re.search(r"\{.*\}", response, re.DOTALL)

            if json_match:
                evaluation = json.loads(json_match.group())

                # Validate required fields
                for criterion in criteria:
                    if criterion not in evaluation:
                        evaluation[criterion] = 0.5  # Default score

                # Calculate overall score if not provided
                if "overall_score" not in evaluation:
                    evaluation["overall_score"] = sum(
                        evaluation.get(criterion, 0) for criterion in criteria
                    ) / len(criteria)

                # Record evaluation
                evaluation_record = {
                    "timestamp": datetime.utcnow().isoformat(),
                    "query": query,
                    "result_count": len(results),
                    "evaluation": evaluation,
                }
                self.evaluation_history.append(evaluation_record)

                return evaluation
            else:
                # Fallback evaluation
                return {
                    "relevance": 0.6,
                    "accuracy": 0.6,
                    "completeness": 0.6,
                    "context": 0.6,
                    "overall_score": 0.6,
                    "analysis": "Unable to parse LLM response, using fallback scores",
                    "recommendations": "Improve response format parsing",
                }

        except Exception as e:
            print(f"LLM Judge evaluation failed: {e}")
            return {
                "relevance": 0.5,
                "accuracy": 0.5,
                "completeness": 0.5,
                "context": 0.5,
                "overall_score": 0.5,
                "analysis": f"Evaluation error: {str(e)}",
                "recommendations": "Fix evaluation system",
            }

    def get_performance_summary(self) -> Dict[str, Any]:
        """Get summary of all LLM judge evaluations."""
        if not self.evaluation_history:
            return {"total_evaluations": 0}

        scores = [
            eval["evaluation"]["overall_score"] for eval in self.evaluation_history
        ]

        return {
            "total_evaluations": len(self.evaluation_history),
            "average_score": sum(scores) / len(scores),
            "min_score": min(scores),
            "max_score": max(scores),
            "evaluations_above_threshold": sum(1 for score in scores if score >= 0.7),
            "performance_trend": scores[-5:] if len(scores) >= 5 else scores,
        }


# Comprehensive test data with rich scenarios
COMPREHENSIVE_TEST_MEMORIES = [
    {
        "simple_content": "Vasilisa celebrated her birthday with close friends and family",
        "full_content": "Vasilisa had a wonderful birthday celebration surrounded by her closest friends and loving family. Daniil was there and helped organize the surprise party. Everyone brought thoughtful gifts and shared happy memories together.",
        "importance": 950,
        "categories": ["social.person", "social.event"],
        "emotion_label": "joy",
        "keywords": [
            "vasilisa",
            "birthday",
            "celebration",
            "friends",
            "family",
            "daniil",
        ],
        "temporal_context": "recent celebration",
    },
    {
        "simple_content": "Panic attack during important client presentation",
        "full_content": "Experienced severe anxiety and panic attack during the quarterly client presentation. Heart racing, difficulty breathing, felt like losing control. Had to step out and take deep breaths. Client was understanding but it was embarrassing.",
        "importance": 900,
        "categories": ["self.emotion", "work.career"],
        "emotion_label": "panic",
        "keywords": ["panic", "anxiety", "presentation", "client", "work"],
        "emotional_context": "work stress",
    },
    {
        "simple_content": "Deep interest in artificial intelligence ethics and bias mitigation",
        "full_content": "Fascinated by the ethical challenges in AI development, particularly algorithmic bias and fairness. Reading research papers on bias mitigation techniques and following discussions about responsible AI deployment. Considering pursuing further education in this area.",
        "importance": 850,
        "categories": ["interest", "technology.ai"],
        "emotion_label": "excitement",
        "keywords": ["AI", "ethics", "bias", "mitigation", "fairness", "research"],
        "interest_topic": "AI ethics and bias",
    },
    {
        "simple_content": "Daniil provided crucial debugging help for complex production issue",
        "full_content": "Daniil was absolutely instrumental in solving a critical production bug. He stayed late, helped trace through complex code paths, and identified the root cause. His expertise and patience were invaluable. This reinforces what a great friend and colleague he is.",
        "importance": 880,
        "categories": ["social.person", "work.technical"],
        "emotion_label": "gratitude",
        "keywords": ["daniil", "debugging", "production", "bug", "help", "friend"],
        "social_context": "professional collaboration",
    },
    {
        "simple_content": "Feeling overwhelmed with multiple project deadlines approaching",
        "full_content": "Stressed about managing three major project deadlines all converging next week. Working long hours, struggling to maintain work-life balance. Feeling pressure to deliver high-quality work while meeting tight timelines. Need better time management strategies.",
        "importance": 750,
        "categories": ["self.emotion", "work.career"],
        "emotion_label": "stress",
        "keywords": ["overwhelmed", "deadlines", "stress", "projects", "work-life"],
        "emotional_context": "work pressure",
    },
    {
        "simple_content": "Learned new machine learning framework during weekend project",
        "full_content": "Spent the entire weekend learning PyTorch and building a small neural network project. Really enjoying the hands-on learning experience. The framework feels intuitive and the documentation is excellent. Made good progress on understanding tensor operations and model training.",
        "importance": 700,
        "categories": ["interest", "technology.ml"],
        "emotion_label": "accomplishment",
        "keywords": [
            "pytorch",
            "machine learning",
            "neural network",
            "learning",
            "project",
        ],
        "interest_topic": "machine learning frameworks",
    },
    {
        "simple_content": "Comforting conversation with Vasilisa after difficult day",
        "full_content": "Had a really challenging day at work and Vasilila called at just the right moment. Her empathy and understanding helped me process everything. She shared similar experiences and offered practical advice. Grateful for such a supportive friend who always knows what to say.",
        "importance": 820,
        "categories": ["social.person", "self.emotion"],
        "emotion_label": "comfort",
        "keywords": ["vasilisa", "comfort", "support", "friend", "empathy"],
        "social_context": "emotional support",
    },
    {
        "simple_content": "Excited about upcoming conference on natural language processing",
        "full_content": "Registered for the NLP conference next month. Looking forward to learning about the latest research in language models, transformers, and semantic search. Several speakers from leading AI companies will be presenting. Planning to network with researchers and practitioners.",
        "importance": 780,
        "categories": ["interest", "technology.nlp"],
        "emotion_label": "excitement",
        "keywords": [
            "conference",
            "NLP",
            "language models",
            "transformers",
            "research",
        ],
        "interest_topic": "natural language processing",
    },
]


@pytest.fixture
async def comprehensive_test_memories(async_session):
    """Create comprehensive test memories with rich scenarios for LLM judge testing."""
    from core.models.memory import Category, Memory, memory_category_association

    # Create categories for comprehensive testing
    categories_data = [
        {
            "name": "person",
            "domain": "social",
            "full_path": "social.person",
            "icon": "ðŸ‘¤",
        },
        {
            "name": "event",
            "domain": "social",
            "full_path": "social.event",
            "icon": "ðŸŽ‰",
        },
        {
            "name": "emotion",
            "domain": "self",
            "full_path": "self.emotion",
            "icon": "ðŸ˜Š",
        },
        {"name": "career", "domain": "work", "full_path": "work.career", "icon": "ðŸ’¼"},
        {
            "name": "technical",
            "domain": "work",
            "full_path": "work.technical",
            "icon": "ðŸ”§",
        },
        {
            "name": "interest",
            "domain": "interest",
            "full_path": "interest",
            "icon": "ðŸŽ¯",
        },
        {
            "name": "ai",
            "domain": "technology",
            "full_path": "technology.ai",
            "icon": "ðŸ¤–",
        },
        {
            "name": "ml",
            "domain": "technology",
            "full_path": "technology.ml",
            "icon": "ðŸ§ ",
        },
        {
            "name": "nlp",
            "domain": "technology",
            "full_path": "technology.nlp",
            "icon": "ðŸ’¬",
        },
    ]

    created_categories = []
    for cat_data in categories_data:
        category = Category(**cat_data)
        async_session.add(category)
        created_categories.append(category)
    await async_session.commit()

    # Create category mapping
    category_map = {cat.full_path: cat for cat in created_categories}

    created_memories = []
    for i, memory_data in enumerate(COMPREHENSIVE_TEST_MEMORIES, 1):
        # Get category IDs
        category_ids = []
        for cat_path in memory_data["categories"]:
            if cat_path in category_map:
                category_ids.append(category_map[cat_path].id)

        # Create memory with rich content
        memory = Memory(
            simple_content=memory_data["simple_content"],
            full_content=memory_data["full_content"],
            importance=memory_data["importance"],
            created_by=123456789,
            emotion_label=memory_data["emotion_label"],
            keywords=memory_data["keywords"],
            embedding=json.dumps(
                [0.1 * i, 0.2 * i, 0.3 * i] * 128
            ),  # Varied embeddings
        )

        async_session.add(memory)
        await async_session.flush()
        memory_id = memory.id

        # Add categories
        for cat_id in category_ids:
            await async_session.execute(
                memory_category_association.insert().values(
                    memory_id=memory_id, category_id=cat_id
                )
            )

        created_memories.append(memory)

    await async_session.commit()
    for memory in created_memories:
        await async_session.refresh(memory, ["categories"])

    return created_memories, created_categories


@pytest.fixture
def llm_judge():
    """Create LLM judge instance."""
    return LLMJudge()


@pytest.fixture
async def tool_executor(async_session):
    """Create tool executor."""
    return ToolExecutor(async_session)


# Complex Query Patterns Tests


@pytest.mark.asyncio
async def test_complex_multi_concept_search_with_llm_judge(
    tool_executor, comprehensive_test_memories, llm_judge
):
    """
    E2E Test: Complex multi-concept search with LLM judge validation.

    Tests queries that combine multiple concepts and relationships:
    - "friends who helped with work problems"
    - "stressful work situations involving technical challenges"
    - "exciting learning experiences in artificial intelligence"
    """
    _, _ = comprehensive_test_memories

    complex_queries = [
        {
            "query": "friends who helped with work problems",
            "expected_concepts": ["friends", "help", "work", "problems"],
            "context": "Looking for supportive colleagues who assisted with professional challenges",
        },
        {
            "query": "stressful work situations involving technical challenges",
            "expected_concepts": ["stress", "work", "technical", "challenges"],
            "context": "Identifying difficult work experiences with technical components",
        },
        {
            "query": "exciting learning experiences in artificial intelligence",
            "expected_concepts": ["excitement", "learning", "AI", "experiences"],
            "context": "Finding positive learning moments related to AI/ML",
        },
    ]

    for query_data in complex_queries:
        # Execute search
        start_time = time.time()
        result = await tool_executor.execute(
            tool_name="semantic_search",
            arguments={"query": query_data["query"], "limit": 10},
            user_id=123456789,
        )
        execution_time = time.time() - start_time

        # Validate basic functionality
        assert result["success"] is True
        # Allow longer execution time due to LLM calls
        assert execution_time < 10.0, (
            f"Search took {execution_time:.3f}s, expected <10.0s"
        )

        # LLM Judge evaluation
        evaluation = await llm_judge.evaluate_search_quality(
            query=query_data["query"],
            results=result["memories"],
            context=query_data["context"],
            criteria=["relevance", "accuracy", "completeness", "context"],
        )

        # LLM should find results satisfactory for complex queries
        assert evaluation["overall_score"] >= 0.4, (
            f"LLM judge score too low for complex query '{query_data['query']}': {evaluation['overall_score']}"
        )

        # Results should contain expected concepts
        if result["memories"]:
            results_text = " ".join(
                [mem["simple_content"].lower() for mem in result["memories"]]
            )
            for concept in query_data["expected_concepts"]:
                assert concept in results_text or len(result["memories"]) == 0, (
                    f"Expected concept '{concept}' not found in results for '{query_data['query']}'"
                )


@pytest.mark.asyncio
async def test_emotional_context_search_with_llm_judge(
    tool_executor, comprehensive_test_memories, llm_judge
):
    """
    E2E Test: Emotional context search with LLM judge validation.

    Tests queries that focus on emotional states and contexts:
    - "moments of stress and anxiety"
    - "times when I felt grateful for friends"
    - "exciting learning achievements"
    - "situations that caused panic or fear"
    """
    _, _ = comprehensive_test_memories

    emotional_queries = [
        {
            "query": "moments of stress and anxiety",
            "emotion_focus": "stress",
            "context": "Finding memories about stressful situations and anxiety",
        },
        {
            "query": "times when I felt grateful for friends",
            "emotion_focus": "gratitude",
            "context": "Identifying moments of thankfulness for friends' support",
        },
        {
            "query": "exciting learning achievements",
            "emotion_focus": "excitement",
            "context": "Finding positive learning experiences with excitement",
        },
        {
            "query": "situations that caused panic or fear",
            "emotion_focus": "panic",
            "context": "Identifying memories of panic attacks or fearful situations",
        },
    ]

    for query_data in emotional_queries:
        # Test search_by_emotion tool
        result = await tool_executor.execute(
            tool_name="search_by_emotion",
            arguments={"emotion": query_data["emotion_focus"]},
            user_id=123456789,
        )

        assert result["success"] is True
        assert "memories" in result

        # LLM Judge evaluation of emotional context
        evaluation = await llm_judge.evaluate_search_quality(
            query=query_data["query"],
            results=result["memories"],
            context=query_data["context"],
            criteria=["relevance", "accuracy", "context"],
        )

        # Emotional search should have good context preservation
        assert evaluation.get("context", 0) >= 0.5, (
            f"Context preservation too low for emotional query '{query_data['query']}': {evaluation.get('context', 0)}"
        )

        assert evaluation["overall_score"] >= 0.5, (
            f"LLM judge score too low for emotional query '{query_data['query']}': {evaluation['overall_score']}"
        )


@pytest.mark.asyncio
async def test_social_relationship_search_with_llm_judge(
    tool_executor, comprehensive_test_memories, llm_judge
):
    """
    E2E Test: Social relationship search with LLM judge validation.

    Tests queries about social connections and relationships:
    - "memories about Vasilisa's support"
    - "times Daniil helped with technical problems"
    - "social events with friends and family"
    - "professional collaboration with colleagues"
    """
    _, _ = comprehensive_test_memories

    social_queries = [
        {
            "tool": "search_by_person",
            "args": {"person_name": "Vasilisa"},
            "query": "memories about Vasilisa's support",
            "context": "Finding memories where Vasilisa provided emotional or practical support",
        },
        {
            "tool": "search_by_person",
            "args": {"person_name": "Daniil"},
            "query": "times Daniil helped with technical problems",
            "context": "Identifying technical assistance from Daniil",
        },
    ]

    for query_data in social_queries:
        # Execute search
        result = await tool_executor.execute(
            tool_name=query_data["tool"],
            arguments=query_data["args"],
            user_id=123456789,
        )

        assert result["success"] is True
        assert "memories" in result

        # LLM Judge evaluation of social context
        evaluation = await llm_judge.evaluate_search_quality(
            query=query_data["query"],
            results=result["memories"],
            context=query_data["context"],
            criteria=["relevance", "accuracy", "context", "completeness"],
        )

        # Social relationship searches should maintain context well
        assert evaluation.get("context", 0) >= 0.5, (
            f"Social context preservation too low: {evaluation.get('context', 0)}"
        )

        assert evaluation["overall_score"] >= 0.5, (
            f"LLM judge score too low for social query '{query_data['query']}': {evaluation['overall_score']}"
        )


@pytest.mark.asyncio
async def test_interest_expertise_search_with_llm_judge(
    tool_executor, comprehensive_test_memories, llm_judge
):
    """
    E2E Test: Interest and expertise search with LLM judge validation.

    Tests queries about interests and specialized knowledge:
    - "artificial intelligence ethics and bias"
    - "machine learning frameworks and neural networks"
    - "natural language processing research"
    - "learning new technical skills"
    """
    _, _ = comprehensive_test_memories

    # Test get_interests tool
    result = await tool_executor.execute(
        tool_name="get_interests", arguments={}, user_id=123456789
    )

    assert result["success"] is True
    assert "interests" in result

    # LLM Judge evaluation of interest search
    evaluation = await llm_judge.evaluate_search_quality(
        query="interests and specialized knowledge areas",
        results=result["interests"],
        context="Evaluating how well the interest search captures specialized knowledge areas",
        criteria=["relevance", "accuracy", "completeness"],
    )

    # Interest search should be accurate and comprehensive
    assert evaluation.get("accuracy", 0) >= 0.6, (
        f"Interest accuracy too low: {evaluation.get('accuracy', 0)}"
    )

    assert evaluation["overall_score"] >= 0.6, (
        f"LLM judge score too low for interest search: {evaluation['overall_score']}"
    )

    # Test specific interest searches
    interest_queries = [
        "artificial intelligence ethics",
        "machine learning frameworks",
        "natural language processing",
    ]

    for interest_query in interest_queries:
        result = await tool_executor.execute(
            tool_name="semantic_search",
            arguments={"query": interest_query, "limit": 5},
            user_id=123456789,
        )

        assert result["success"] is True

        # Quick validation that results are relevant
        if result["memories"]:
            results_text = " ".join(
                [mem["simple_content"].lower() for mem in result["memories"]]
            )
            # Should contain some relevant keywords
            relevance_indicators = [
                "ai",
                "ethics",
                "machine learning",
                "neural",
                "nlp",
                "language",
            ]
            found_relevant = any(
                indicator in results_text for indicator in relevance_indicators
            )
            assert found_relevant or len(result["memories"]) == 0, (
                f"No relevant indicators found for interest query '{interest_query}'"
            )


# Performance Stress Tests with LLM Judge


@pytest.mark.asyncio
async def test_performance_stress_with_llm_judge(
    tool_executor, comprehensive_test_memories, llm_judge
):
    """
    E2E Test: Performance stress testing with LLM judge validation.

    Tests performance under load with quality validation:
    - Multiple rapid searches
    - Complex queries with multiple filters
    - Cross-tool performance comparison
    - Memory usage monitoring
    """
    _, _ = comprehensive_test_memories

    # Define stress test scenarios
    stress_scenarios = [
        {
            "name": "Rapid Semantic Searches",
            "tool": "semantic_search",
            "queries": [
                "friends and family",
                "work stress and anxiety",
                "learning and development",
                "technical challenges",
                "emotional support",
            ],
        },
        {
            "name": "Specialized Tools Performance",
            "tools": [
                {"name": "get_all_friends", "args": {}},
                {"name": "get_panic_attacks", "args": {}},
                {"name": "get_interests", "args": {}},
                {"name": "search_by_person", "args": {"person_name": "Vasilisa"}},
                {"name": "search_by_emotion", "args": {"emotion": "excitement"}},
            ],
        },
    ]

    performance_results = []

    # Test rapid semantic searches
    scenario = stress_scenarios[0]
    print(f"\n=== Testing {scenario['name']} ===")

    for query in scenario["queries"]:
        start_time = time.time()

        result = await tool_executor.execute(
            tool_name=scenario["tool"],
            arguments={"query": query, "limit": 10},
            user_id=123456789,
        )

        execution_time = time.time() - start_time

        # LLM Judge evaluation for performance test
        evaluation = await llm_judge.evaluate_search_quality(
            query=query,
            results=result["memories"],
            context=f"Performance test query: {query}",
            criteria=["relevance", "accuracy"],
        )

        performance_result = {
            "query": query,
            "execution_time": execution_time,
            "result_count": len(result.get("memories", [])),
            "llm_score": evaluation["overall_score"],
            "success": result["success"],
        }

        performance_results.append(performance_result)

        # Performance assertion
        assert execution_time < 0.5, (
            f"Query '{query}' took {execution_time:.3f}s, expected <0.5s"
        )
        assert result["success"] is True, (
            f"Query '{query}' failed: {result.get('error', 'Unknown error')}"
        )

        print(
            f"âœ… '{query}' - {execution_time:.3f}s (LLM: {evaluation['overall_score']:.2f})"
        )

    # Test specialized tools performance
    scenario = stress_scenarios[1]
    print(f"\n=== Testing {scenario['name']} ===")

    for tool_config in scenario["tools"]:
        start_time = time.time()

        result = await tool_executor.execute(
            tool_name=tool_config["name"],
            arguments=tool_config["args"],
            user_id=123456789,
        )

        execution_time = time.time() - start_time

        performance_result = {
            "tool": tool_config["name"],
            "execution_time": execution_time,
            "result_count": len(
                result.get(list(result.keys())[1], [])
            ),  # Get result count
            "success": result["success"],
        }

        performance_results.append(performance_result)

        # Performance assertion
        assert execution_time < 0.5, (
            f"Tool '{tool_config['name']}' took {execution_time:.3f}s, expected <0.5s"
        )
        assert result["success"] is True, (
            f"Tool '{tool_config['name']}' failed: {result.get('error', 'Unknown error')}"
        )

        print(f"âœ… '{tool_config['name']}' - {execution_time:.3f}s")

    # Analyze performance results
    avg_execution_time = sum(r["execution_time"] for r in performance_results) / len(
        performance_results
    )
    max_execution_time = max(r["execution_time"] for r in performance_results)
    llm_scores = [
        r.get("llm_score", 0) for r in performance_results if "llm_score" in r
    ]
    avg_llm_score = sum(llm_scores) / len(llm_scores) if llm_scores else 0

    print("\n=== Performance Summary ===")
    print(f"Average execution time: {avg_execution_time:.3f}s")
    print(f"Maximum execution time: {max_execution_time:.3f}s")
    print(f"Average LLM judge score: {avg_llm_score:.2f}")
    print(f"Total operations: {len(performance_results)}")
    print("========================\n")

    # Performance quality assertions
    assert avg_execution_time < 0.3, (
        f"Average execution time too high: {avg_execution_time:.3f}s"
    )
    assert max_execution_time < 0.5, (
        f"Maximum execution time too high: {max_execution_time:.3f}s"
    )
    assert avg_llm_score >= 0.5, f"Average LLM judge score too low: {avg_llm_score:.2f}"


# Cross-Tool Workflow Tests


@pytest.mark.asyncio
async def test_cross_tool_search_workflows_with_llm_judge(
    tool_executor, comprehensive_test_memories, llm_judge
):
    """
    E2E Test: Cross-tool search workflows with LLM judge validation.

    Tests realistic workflows that combine multiple search tools:
    - Emotional support workflow: Find friends who provided emotional support during stress
    - Professional growth workflow: Find learning experiences related to work challenges
    - Social network workflow: Analyze social connections and support patterns
    """
    _, _ = comprehensive_test_memories

    # Workflow 1: Emotional Support Analysis
    print("=== Testing Emotional Support Workflow ===")

    # Step 1: Find stressful situations
    stress_results = await tool_executor.execute(
        tool_name="search_by_emotion",
        arguments={"emotion": "stress"},
        user_id=123456789,
    )

    assert stress_results["success"] is True

    # Step 2: Find friends who provided support
    friends_results = await tool_executor.execute(
        tool_name="get_all_friends", arguments={}, user_id=123456789
    )

    assert friends_results["success"] is True

    # Step 3: Search for specific supportive interactions
    support_results = await tool_executor.execute(
        tool_name="semantic_search",
        arguments={"query": "emotional support comfort during difficult times"},
        user_id=123456789,
    )

    assert support_results["success"] is True

    # LLM Judge evaluation of workflow
    all_results = (
        stress_results.get("panic_attacks", [])
        + friends_results.get("friends", [])
        + support_results.get("memories", [])
    )

    workflow_evaluation = await llm_judge.evaluate_search_quality(
        query="emotional support patterns and stress coping mechanisms",
        results=all_results,
        context="Analyzing how I deal with stress and who provides emotional support",
        criteria=["relevance", "accuracy", "completeness", "context"],
    )

    print(
        f"Emotional Support Workflow LLM Score: {workflow_evaluation['overall_score']:.2f}"
    )
    assert workflow_evaluation["overall_score"] >= 0.5, (
        f"Emotional support workflow quality too low: {workflow_evaluation['overall_score']}"
    )

    # Workflow 2: Professional Growth Analysis
    print("=== Testing Professional Growth Workflow ===")

    # Step 1: Find technical challenges
    tech_results = await tool_executor.execute(
        tool_name="semantic_search",
        arguments={"query": "technical challenges debugging problem solving"},
        user_id=123456789,
    )

    # Step 2: Find learning experiences
    learning_results = await tool_executor.execute(
        tool_name="semantic_search",
        arguments={"query": "learning new skills professional development"},
        user_id=123456789,
    )

    # Step 3: Find colleagues who helped
    colleague_results = await tool_executor.execute(
        tool_name="search_by_person",
        arguments={"person_name": "Daniil"},
        user_id=123456789,
    )

    # LLM Judge evaluation of professional growth workflow
    professional_results = (
        tech_results.get("memories", [])
        + learning_results.get("memories", [])
        + colleague_results.get("memories", [])
    )

    professional_evaluation = await llm_judge.evaluate_search_quality(
        query="professional growth patterns and learning opportunities",
        results=professional_results,
        context="Analyzing professional development, challenges, and collaborative learning",
        criteria=["relevance", "accuracy", "completeness"],
    )

    print(
        f"Professional Growth Workflow LLM Score: {professional_evaluation['overall_score']:.2f}"
    )
    assert professional_evaluation["overall_score"] >= 0.5, (
        f"Professional growth workflow quality too low: {professional_evaluation['overall_score']}"
    )

    # Workflow 3: Interest and Expertise Analysis
    print("=== Testing Interest Analysis Workflow ===")

    # Step 1: Get all interests
    interests_results = await tool_executor.execute(
        tool_name="get_interests", arguments={}, user_id=123456789
    )

    # Step 2: Search for AI/ML related content
    ai_results = await tool_executor.execute(
        tool_name="semantic_search",
        arguments={"query": "artificial intelligence machine learning expertise"},
        user_id=123456789,
    )

    # LLM Judge evaluation of interest workflow
    interest_workflow_results = interests_results.get("interests", []) + ai_results.get(
        "memories", []
    )

    interest_evaluation = await llm_judge.evaluate_search_quality(
        query="interests and expertise areas particularly in AI/ML",
        results=interest_workflow_results,
        context="Analyzing personal interests and areas of expertise development",
        criteria=["relevance", "accuracy", "completeness"],
    )

    print(
        f"Interest Analysis Workflow LLM Score: {interest_evaluation['overall_score']:.2f}"
    )
    assert interest_evaluation["overall_score"] >= 0.5, (
        f"Interest analysis workflow quality too low: {interest_evaluation['overall_score']}"
    )


# Edge Cases and Boundary Conditions


@pytest.mark.asyncio
async def test_edge_cases_with_llm_judge(
    tool_executor, comprehensive_test_memories, llm_judge
):
    """
    E2E Test: Edge cases and boundary conditions with LLM judge validation.

    Tests unusual scenarios and edge cases:
    - Empty or minimal queries
    - Very specific or niche queries
    - Contradictory search terms
    - Queries that should return no results
    - High importance vs low importance filtering
    """
    _, _ = comprehensive_test_memories

    edge_cases = [
        {
            "name": "Empty query",
            "tool": "semantic_search",
            "args": {"query": "", "limit": 5},
            "should_succeed": False,
        },
        {
            "name": "Nonexistent person",
            "tool": "search_by_person",
            "args": {"person_name": "NonexistentPerson123"},
            "should_succeed": True,
            "expected_empty": True,
        },
        {
            "name": "Minimal query",
            "tool": "semantic_search",
            "args": {"query": "AI", "limit": 3},
            "should_succeed": True,
        },
        {
            "name": "Very specific niche query",
            "tool": "semantic_search",
            "args": {"query": "PyTorch tensor operations debugging", "limit": 5},
            "should_succeed": True,
        },
        {
            "name": "High importance filter",
            "tool": "semantic_search",
            "args": {"query": "experiences", "min_importance": 900, "limit": 5},
            "should_succeed": True,
        },
    ]

    for edge_case in edge_cases:
        print(f"Testing edge case: {edge_case['name']}")

        result = await tool_executor.execute(
            tool_name=edge_case["tool"], arguments=edge_case["args"], user_id=123456789
        )

        if edge_case["should_succeed"]:
            assert result["success"] is True, (
                f"Edge case '{edge_case['name']}' should succeed"
            )

            if edge_case.get("expected_empty"):
                assert len(result.get("memories", [])) == 0, (
                    f"Edge case '{edge_case['name']}' should return empty results"
                )

            # LLM Judge evaluation for edge cases that succeed
            if result.get("memories"):
                evaluation = await llm_judge.evaluate_search_quality(
                    query=edge_case["args"].get("query", "edge case query"),
                    results=result["memories"],
                    context=f"Edge case test: {edge_case['name']}",
                    criteria=["relevance", "accuracy"],
                )

                print(f"  LLM Score: {evaluation['overall_score']:.2f}")

                # Edge cases can have lower quality scores but should still be reasonable
                assert evaluation["overall_score"] >= 0.3, (
                    f"Edge case '{edge_case['name']}' quality too low: {evaluation['overall_score']}"
                )
        else:
            assert result["success"] is False, (
                f"Edge case '{edge_case['name']}' should fail"
            )
            print(f"  Correctly failed with: {result.get('error', 'Unknown error')}")

        print(f"  âœ… {edge_case['name']} handled correctly")


@pytest.mark.asyncio
async def test_comprehensive_llm_judge_summary(llm_judge):
    """
    E2E Test: Generate comprehensive LLM judge evaluation summary.

    Provides overall assessment of PRP-007 search quality based on
    all LLM judge evaluations performed during testing.
    """
    summary = llm_judge.get_performance_summary()

    print("\n" + "=" * 60)
    print("COMPREHENSIVE LLM JUDGE EVALUATION SUMMARY")
    print("=" * 60)
    print(f"Total Evaluations: {summary['total_evaluations']}")
    print(f"Average Score: {summary['average_score']:.3f}")
    print(f"Score Range: {summary['min_score']:.3f} - {summary['max_score']:.3f}")
    print(
        f"Evaluations Above Threshold (â‰¥0.7): {summary['evaluations_above_threshold']}"
    )

    if summary["total_evaluations"] > 0:
        quality_ratio = (
            summary["evaluations_above_threshold"] / summary["total_evaluations"]
        )
        print(f"Quality Ratio: {quality_ratio:.2%}")

        # Performance trend analysis
        if len(summary["performance_trend"]) > 1:
            trend_scores = summary["performance_trend"]
            avg_first_half = sum(trend_scores[: len(trend_scores) // 2]) / (
                len(trend_scores) // 2
            )
            avg_second_half = sum(trend_scores[len(trend_scores) // 2 :]) / (
                len(trend_scores) // 2
            )

            if avg_second_half > avg_first_half:
                print("Performance Trend: ðŸ“ˆ Improving")
            elif avg_second_half < avg_first_half:
                print("Performance Trend: ðŸ“‰ Declining")
            else:
                print("Performance Trend: âž¡ï¸ Stable")

    print("=" * 60)

    # Final quality assertions
    assert summary["total_evaluations"] >= 5, (
        f"Insufficient LLM evaluations: {summary['total_evaluations']}"
    )

    assert summary["average_score"] >= 0.6, (
        f"Average LLM score too low: {summary['average_score']}"
    )

    quality_ratio = (
        summary["evaluations_above_threshold"] / summary["total_evaluations"]
    )
    assert quality_ratio >= 0.6, (
        f"Too few evaluations above quality threshold: {quality_ratio:.2%}"
    )

    print("\nðŸŽ‰ COMPREHENSIVE E2E TESTING COMPLETE")
    print("âœ… All LLM judge evaluations passed")
    print("âœ… Performance requirements met")
    print("âœ… Quality standards achieved")
    print("âœ… PRP-007 validated with LLM judge")
    print("=" * 60)


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--llm-judge"])
